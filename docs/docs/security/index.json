[{"url":"/docs/security/data-source/","title":"At the data source","content":"Securing the Search data sources Search is backend agnostic by design; this means we do not suggest a specific implementation as correct. On this page, we will attempt to bring up some options for you to consider.\nUsing access controls of the data source Whether your data is stored in a database like MySQL and PostgreSQL, or hosted solutions like Google Cloud Storage, the database will likely offer some form of access control. The access control can be broad or fine-grained, extending to table/column/row-level security. Your implementation can leverage the database\u0026rsquo;s access controls to implement GA4GH Passport and Visas to secure your data.\nNot familiar with GA4GH Passports and Visas?\nUsing Presto’s system access control If your implementation uses Presto, you can use Presto\u0026rsquo;s system access control to secure your data source.\nMake a copy of your data If you plan to share a select set of data, you can consider making a copy of all the shared data in a different database/storage instance. Not having sensitive data in your shared data source at all is the most secure solution. If the plan is to share a specific dataset publicly, this is the best option to avoid implementing complicated filters.\n"},{"url":"/docs/security/search-endpoint/","title":"At the search endpoint","content":"Securing the Search endpoint The endpoint should be secured according to the GA4GH Passports and Visas standard as suggested by the GA4GH Data Use and Researcher ID workstream.\n"},{"url":"/docs/getting-started/introduction/","title":"Introduction","content":"{row-divider}\nThe GA4GH Search API The GA4GH Search API specification describes a simple, uniform mechanism to publish, discover, query, and analyze biomedical data. Any “rectangular” data that fits into rows and columns can be represented by GA4GH Search.\nSearch API for data custodians Search API is a perfect solution for data custodians looking to make their biomedical data discoverable and searchable.\n The API is minimalistic by design, which also means minimal resistance to adoption. Search does not prescribe a particular data model. If it fits into rows and columns, you can publish it. Search serves as a general-purpose framework for building federative search-based applications across multiple implementations. Search is backend agnostic. It is possible to implement the API across a large variety of backend datastores.  Search API for data consumers Search API is a perfect solution for data consumers looking to discover and search biomedical data in an interoperable way.\n Search API is RESTful. Read our Open API 3 specification. Search API is discoverable and browsable. See supported table operations Search API is queryable and familiar. Search API\u0026rsquo;s SQL dialect has a familiar interface inspired by current major open source database platforms.  {divider} Quick Links  Full API Specifications\nInstalling Client Libraries\nPublishing Data Examples\nData Consumption Examples\n \n{row-divider}\nInstalling Client Libraries Search has client libraries for R and Python, as well as a command-line interface. We’ll be using these client libraries in the following examples. {divider}  Python R CLI   # Installing the client library form PyPi pip install search-python-client # Installing from Github pip install git+https://github.com/DNAstack/search-python-client --no-cache-dir   # Setup devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;)   This CLI requires Java 11+ on your system\ncurl https://storage.googleapis.com/ga4gh-search-cli/tables-cli-2.1-55-gc484f8b-executable.jar \u0026gt; search-cli chmod +x search-cli mv search-cli /usr/local/bin # (somewhere on your search path) search-cli --version You should see:\ntables-api-cli Version : 1.0-0.2.1-55-gc484f8b    \n"},{"url":"/docs/reference/pagination-long-queries/","title":"Pagination and Long Running Queries","content":"Pagination sequence\nA pagination sequence is the singly-linked list of URLs formed by following the next_page_url property of the pagination section of an initial TableData or ListTablesResponse. A pagination sequence begins at the first response returned from any request that yields a TableData or ListTablesResponse, and ends at the page in the sequence whose pagination property is omitted, whose pagination.next_page_url is omitted, or whose pagination.next_page_url is null.\nServers may return a unique pagination sequence in response to successive requests for the same query, table data listing, or table listing.\nExcept for the last page, pagination.next_page_url property must be either an absolute URL or a relative reference as defined by RFC 3986 section 4.2 whose base URL is the URL that the page containing the reference was fetched from.\nEvery non-empty TableData page in a pagination sequence must include a data_model property. If present, the data_model property must be a valid JSON Schema.\nAcross all TableData pages in the pagination sequence that have a data_model value, the data_models must be identical. Some TableData pages may lack a data_model. See the empty page rules below.\nServers may respond with an HTTP 4xx error code if the same page is requested more than once.\nDue to both rules above, clients must not rely on the ability to re-fetch previously encountered pages.\nServers may include a Retry-After HTTP header in each response that is part of a pagination sequence, and clients must respect the delay specified by such header before attempting to fetch the next page.\nEmpty TableData pages\nWhile many types of queries will be completed quickly, others will take minutes or even hours to yield a result. The simplest solution would be a synchronous design: query requests block until data is ready, then return a TableData response with the initial rows of the result set. However, asking clients to block for hours on a single HTTP response is fraught with difficulty: open connections are costly and fragile. If an intermediary times out the request, the results will be lost and the client must start over.\nTo allow servers to direct clients to poll for results rather than hold open HTTP connections for long-running queries, the following special pagination rules apply to empty pages.\nAn empty page is defined as a TableData object whose data property is a zero element array.\nA pagination sequence MAY include any number of empty pages anywhere in the sequence.\nAn empty TableData page may omit its data_model property entirely. This allows servers to direct clients to poll for results before the result schema has been determined.\nA server that returns an empty page should include a Retry-After header in the HTTP response. If a client encounters an empty page with no Retry-After header, the client should delay at least 1 second before requesting the next page.\nExample: Server returning empty pages to make client poll\nThis example illustrates a server returning a series of empty pages to a client while it is preparing the result set. The client polls for results by following next_page_url at the rate specified by the server. The form of the pagination URLs are only an example of one possible scheme. Servers are free to employ any pagination URL scheme.\nInitial Request\nPOST /search content-type: application/json {\u0026#34;query\u0026#34;:\u0026#34;select distinct gene_symbol from example_project.brca_exchange.v32\u0026#34;} HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/queued/1\u0026#34;}} 2nd request (Polling after sleeping for 1000ms)\nGET /search/v1/statement/abc123/queued/1 HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/queued/2\u0026#34;}} 3rd request (Polling again after sleeping for 1000ms)\nGET /search/v1/statement/abc123/queued/2 HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/executing/1\u0026#34;}} 4th request (Polling again after sleeping for 1000ms)\nGET /search/v1/statement/abc123/executing/1 HTTP/1.1 200 OK content-type: application/json {\u0026#34;data_model\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Automatically generated schema\u0026#34;,\u0026#34;$schema\u0026#34;:\u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;gene_symbol\u0026#34;:{\u0026#34;format\u0026#34;:\u0026#34;varchar\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}}},\u0026#34;data\u0026#34;:[{\u0026#34;gene_symbol\u0026#34;:\u0026#34;BRCA2\u0026#34;},{\u0026#34;gene_symbol\u0026#34;:\u0026#34;BRCA1\u0026#34;}],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/executing/2\u0026#34;}} Final request (no delay because page was nonempty and no retry-after header was present on the response)\nGET /search/v1/statement/abc123/executing/2 HTTP/1.1 200 OK content-type: application/json {\u0026#34;data_model\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Automatically generated schema\u0026#34;,\u0026#34;$schema\u0026#34;:\u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;gene_symbol\u0026#34;:{\u0026#34;format\u0026#34;:\u0026#34;varchar\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}}},\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{}} Example: Client algorithm for consuming TableData pages\nThe algorithm provided here simply illustrates one way to comply with the rules above. Any algorithm that satisfies all rules acceptable.\n Start with an empty data buffer and undefined data model. Loop:  If the response is an error, report the error and abort If no data_model has been seen so far, check if this page contains a data_model. If so, define the data model for the whole pagination sequence as this page’s data_model. Append the row data from the current page to the data buffer (there may be 0 rows on any given page) Delay for the time specified in the Retry-After HTTP response header for the current page (default is no delay) If there is a pagination object and it has a non-null next_page_url, fetch that URL, make that response the current page, and start back at step 2a; otherwise end.    "},{"url":"/docs/use-exisitng-data/retrofit-a-data-explorer/doc/","title":"Retrofit data explorers","content":"{row-divider}\nMSSNG We\u0026rsquo;re working on it!\n{divider}  Files     ├── content│ ├── category folder│ │ ├── sub-category folder│ │ │ └──_index.md│ │ └── _index.md│ ├── another category |     The code snippet is pulled from this page\n"},{"url":"/docs/reference/sql-functions/","title":"SQL Functions","content":"GA4GH Search API’s SQL dialect has been selected for compatibility with current major open source database platforms including Presto SQL, PostgreSQL, and MySQL, and BigQuery. There are occasional name or signature differences, but a GA4GH Search API implementation atop any of the major database platforms should be able to pass through queries that use the functions listed below with only minor tweaks.\nThe functions below are a subset of those available in PrestoSQL 341. In a conformant GA4GH Search API implementation, these functions must behave according to the Presto documentation. To assist with implementations directly on other database platforms, the PrestoSQL Functions Support Matrix captures the differences between platforms in granular detail.\n Logical Operators  AND, OR, NOT   Comparison Operators  \u0026amp;lt;, \u0026gt;, \u0026amp;lt;=, \u0026gt;=, =, \u0026amp;lt;\u0026gt;, != BETWEEN, IS NULL, IS NOT NULL IS DISTINCT FROM* IS NOT DISTINCT FROM* GREATEST, LEAST Quantified Comparison Predicates: ALL, ANY and SOME* Pattern Comparison: LIKE   Conditional Expressions  CASE, IF, COALESCE, NULLIF   Conversion Functions  cast(value AS type) → type format(format, args...) → varchar   Mathematical Functions  Most basic functions are supported across implementations. Notably missing are hyperbolic trig functions, infinity, floating point, and statistical/CDF functions. abs(x) → [same as input] ceil(x) → [same as input] ceiling(x) → [same as input] degrees(x) → double* exp(x) → double floor(x) → [same as input] ln(x) → double log(b, x) → double log10(x) → double mod(n, m) → [same as input] pi() → double pow(x, p) → double* power(x, p) → double radians(x) → double* round(x) → [same as input] round(x, d) → [same as input] sign(x) → [same as input] sqrt(x) → double truncate(x) → double* Random Functions:  rand() → double* random() → double* random(n) → [same as input]* random(m, n) → [same as input]*   Trigonometric Functions:  acos(x) → double asin(x) → double atan(x) → double atan2(y, x) → double cos(x) → double sin(x) → double tan(x) → double     Bitwise Functions  bitwise_and(x, y) → bigint bitwise_or(x, y) → bigint bitwise_xor(x, y) → bigint bitwise_not(x) → bigint bitwise_left_shift(value, shift) → [same as value] bitwise_right_shift(value, shift, digits) → [same as value] bit_count(x, bits) → bigint*   Regular Expression Functions  regexp_extract_all(string, pattern) -\u0026gt; array(varchar)* regexp_extract_all(string, pattern, group) -\u0026gt; array(varchar)* regexp_extract(string, pattern) → varchar* regexp_extract(string, pattern, group) → varchar* regexp_like(string, pattern) → boolean* regexp_replace(string, pattern) → varchar* regexp_replace(string, pattern, replacement) → varchar* regexp_replace(string, pattern, function) → varchar*   UUID Functions  uuid()*   Session Information Functions  current_user*   String manipulation  Operators:  Concatenation (||)* LIKE   Functions:  chr(n) → varchar* codepoint(string) → integer* format(format, args...) → varchar length(string) → bigint lower(string) → varchar lpad(string, size, padstring) → varchar ltrim(string) → varchar position(substring IN string) → bigint* replace(string, search, replace) → varchar reverse(string) → varchar rpad(string, size, padstring) → varchar rtrim(string) → varchar split(string, delimiter, limit) -\u0026gt; array(varchar)* starts_with(string, substring) → boolean* strpos(string, substring) → bigint* substr(string, start) → varchar* substring(string, start) → varchar substr(string, start, length) → varchar* substring(string, start, length) → varchar trim(string) → varchar upper(string) → varchar     Date manipulation   Be aware of different quotation (') syntax requirements between MySQL and PostgreSQL. BigQuery does not support the +/- operators for dates. Convenience methods could be replaced with EXTRACT().\n  Operators:  +, - * AT TIME ZONE*   Functions:  current_date current_time current_timestamp current_timestamp(p)* date(x) → date* date_trunc(unit, x) → [same as input]* date_add(unit, value, timestamp) → [same as input]* date_diff(unit, timestamp1, timestamp2) → bigint* extract(field FROM x) → bigint* from_unixtime(unixtime) -\u0026gt; timestamp(3)* from_unixtime(unixtime, zone) → timestamp(3) with time zone* from_unixtime(unixtime, hours, minutes) → timestamp(3) with time zone* Localtime* localtimestamp* localtimestamp(p)* now() → timestamp(3) with time zone* to_unixtime(timestamp) → double*   MySQL-like date functions:  date_format(timestamp, format) → varchar* date_parse(string, format) → timestamp(3)*   Aggregate functions **Note that Presto provides a much larger superset of functions. Bitwise, map, and approximate aggregations are mostly absent. Only BigQuery has a few native approximate aggregation functions.  array_agg(x) → array\u0026amp;lt;[same as input]\u0026gt;* avg(x) → double bool_and(boolean) → boolean* bool_or(boolean) → boolean* count(*) → bigint* count(x) → bigint count_if(x) → bigint* every(boolean) → boolean* max(x) → [same as input] max(x, n) → array\u0026amp;lt;[same as x]\u0026gt;* min(x) → [same as input] min(x, n) → array\u0026amp;lt;[same as x]\u0026gt;* sum(x) → [same as input] Statistical Aggregate Functions:  corr(y, x) → double* covar_pop(y, x)→ double* covar_samp(y, x) → double* stddev(x) → double stddev_pop(x) → double stddev_samp(x) → double variance(x) → double var_pop(x) → double var_samp(x) → double     Window functions  Ranking Functions:  cume_dist() → bigint dense_rank() → bigint ntile(n) → bigint percent_rank() → double rank() → bigint row_number() → bigint   Value Functions:  first_value(x) → [same as input] last_value(x) → [same as input] nth_value(x, offset) → [same as input] lead(x[, offset[, default_value]]) → [same as input] lag(x[, offset[, default_value]]) → [same as input]     JSON functions In general, function signatures and behaviour differs across implementations for many JSON related functions.  json_array_length(json) → bigint* json_extract(json, json_path) → json* json_extract_scalar(json, json_path) → varchar* json_format(json) → varchar* json_size(json, json_path) → bigint*   Functions for working with nested and repeated data (ROW and ARRAY) See also UNNEST, which is part of the SQL grammar and allows working with nested arrays as if they were rows in a joined table.  Note: Arrays are mostly absent in MySQL * Array Subscript Operator: [] * Array Concatenation Operator: || * concat(array1, array2, ..., arrayN) → array * cardinality(x) → bigint*\n ga4gh_type (described above)  "},{"url":"/docs/reference/sql-grammar/","title":"SQL Grammar","content":"This is the ANTLR grammar from Presto SQL version 323 (ASL 2.0 license), with the DML and DDL parts removed.\ngrammar DiscoverySearch; tokens { DELIMITER } singleStatement : statement EOF ; standaloneExpression : expression EOF ; standaloneType : type EOF ; statement : query #statementDefault | USE schema=identifier #use | USE catalog=identifier \u0026#39;.\u0026#39; schema=identifier #use | EXPLAIN ANALYZE? VERBOSE? (\u0026#39;(\u0026#39; explainOption (\u0026#39;,\u0026#39; explainOption)* \u0026#39;)\u0026#39;)? statement #explain | SHOW TABLES ((FROM | IN) qualifiedName)? (LIKE pattern=string (ESCAPE escape=string)?)? #showTables | SHOW SCHEMAS ((FROM | IN) identifier)? (LIKE pattern=string (ESCAPE escape=string)?)? #showSchemas | SHOW CATALOGS (LIKE pattern=string)? #showCatalogs | SHOW COLUMNS (FROM | IN) qualifiedName #showColumns | DESCRIBE qualifiedName #showColumns | DESC qualifiedName #showColumns | SHOW FUNCTIONS #showFunctions ; query : with? queryNoWith ; with : WITH RECURSIVE? namedQuery (\u0026#39;,\u0026#39; namedQuery)* ; queryNoWith: queryTerm (ORDER BY sortItem (\u0026#39;,\u0026#39; sortItem)*)? (OFFSET offset=INTEGER_VALUE (ROW | ROWS)?)? ((LIMIT limit=(INTEGER_VALUE | ALL)) | (FETCH (FIRST | NEXT) (fetchFirst=INTEGER_VALUE)? (ROW | ROWS) (ONLY | WITH TIES)))? ; queryTerm : queryPrimary #queryTermDefault | left=queryTerm operator=INTERSECT setQuantifier? right=queryTerm #setOperation | left=queryTerm operator=(UNION | EXCEPT) setQuantifier? right=queryTerm #setOperation ; queryPrimary : querySpecification #queryPrimaryDefault | TABLE qualifiedName #table | VALUES expression (\u0026#39;,\u0026#39; expression)* #inlineTable | \u0026#39;(\u0026#39; queryNoWith \u0026#39;)\u0026#39; #subquery ; sortItem : expression ordering=(ASC | DESC)? (NULLS nullOrdering=(FIRST | LAST))? ; querySpecification : SELECT setQuantifier? selectItem (\u0026#39;,\u0026#39; selectItem)* (FROM relation (\u0026#39;,\u0026#39; relation)*)? (WHERE where=booleanExpression)? (GROUP BY groupBy)? (HAVING having=booleanExpression)? ; groupBy : setQuantifier? groupingElement (\u0026#39;,\u0026#39; groupingElement)* ; groupingElement : groupingSet #singleGroupingSet | ROLLUP \u0026#39;(\u0026#39; (expression (\u0026#39;,\u0026#39; expression)*)? \u0026#39;)\u0026#39; #rollup | CUBE \u0026#39;(\u0026#39; (expression (\u0026#39;,\u0026#39; expression)*)? \u0026#39;)\u0026#39; #cube | GROUPING SETS \u0026#39;(\u0026#39; groupingSet (\u0026#39;,\u0026#39; groupingSet)* \u0026#39;)\u0026#39; #multipleGroupingSets ; groupingSet : \u0026#39;(\u0026#39; (expression (\u0026#39;,\u0026#39; expression)*)? \u0026#39;)\u0026#39; | expression ; namedQuery : name=identifier (columnAliases)? AS \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; ; setQuantifier : DISTINCT | ALL ; selectItem : expression (AS? identifier)? #selectSingle | primaryExpression \u0026#39;.\u0026#39; ASTERISK (AS columnAliases)? #selectAll | ASTERISK #selectAll ; relation : left=relation ( CROSS JOIN right=sampledRelation | joinType JOIN rightRelation=relation joinCriteria | NATURAL joinType JOIN right=sampledRelation ) #joinRelation | sampledRelation #relationDefault ; joinType : INNER? | LEFT OUTER? | RIGHT OUTER? | FULL OUTER? ; joinCriteria : ON booleanExpression | USING \u0026#39;(\u0026#39; identifier (\u0026#39;,\u0026#39; identifier)* \u0026#39;)\u0026#39; ; sampledRelation : aliasedRelation ( TABLESAMPLE sampleType \u0026#39;(\u0026#39; percentage=expression \u0026#39;)\u0026#39; )? ; sampleType : BERNOULLI | SYSTEM ; aliasedRelation : relationPrimary (AS? identifier columnAliases?)? ; columnAliases : \u0026#39;(\u0026#39; identifier (\u0026#39;,\u0026#39; identifier)* \u0026#39;)\u0026#39; ; relationPrimary : qualifiedName #tableName | \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #subqueryRelation | UNNEST \u0026#39;(\u0026#39; expression (\u0026#39;,\u0026#39; expression)* \u0026#39;)\u0026#39; (WITH ORDINALITY)? #unnest | LATERAL \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #lateral | \u0026#39;(\u0026#39; relation \u0026#39;)\u0026#39; #parenthesizedRelation ; expression : booleanExpression ; booleanExpression : valueExpression predicate[$valueExpression.ctx]? #predicated | NOT booleanExpression #logicalNot | left=booleanExpression operator=AND right=booleanExpression #logicalBinary | left=booleanExpression operator=OR right=booleanExpression #logicalBinary ; // workaround for https://github.com/antlr/antlr4/issues/780 predicate[ParserRuleContext value] : comparisonOperator right=valueExpression #comparison | comparisonOperator comparisonQuantifier \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #quantifiedComparison | NOT? BETWEEN lower=valueExpression AND upper=valueExpression #between | NOT? IN \u0026#39;(\u0026#39; expression (\u0026#39;,\u0026#39; expression)* \u0026#39;)\u0026#39; #inList | NOT? IN \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #inSubquery | NOT? LIKE pattern=valueExpression (ESCAPE escape=valueExpression)? #like | IS NOT? NULL #nullPredicate | IS NOT? DISTINCT FROM right=valueExpression #distinctFrom ; valueExpression : primaryExpression #valueExpressionDefault | valueExpression AT timeZoneSpecifier #atTimeZone | operator=(MINUS | PLUS) valueExpression #arithmeticUnary | left=valueExpression operator=(ASTERISK | SLASH | PERCENT) right=valueExpression #arithmeticBinary | left=valueExpression operator=(PLUS | MINUS) right=valueExpression #arithmeticBinary | left=valueExpression CONCAT right=valueExpression #concatenation ; primaryExpression : NULL #nullLiteral | interval #intervalLiteral | identifier string #typeConstructor | DOUBLE PRECISION string #typeConstructor | number #numericLiteral | booleanValue #booleanLiteral | string #stringLiteral | BINARY_LITERAL #binaryLiteral | \u0026#39;?\u0026#39; #parameter | POSITION \u0026#39;(\u0026#39; valueExpression IN valueExpression \u0026#39;)\u0026#39; #position | \u0026#39;(\u0026#39; expression (\u0026#39;,\u0026#39; expression)+ \u0026#39;)\u0026#39; #rowConstructor | ROW \u0026#39;(\u0026#39; expression (\u0026#39;,\u0026#39; expression)* \u0026#39;)\u0026#39; #rowConstructor | qualifiedName \u0026#39;(\u0026#39; ASTERISK \u0026#39;)\u0026#39; filter? over? #functionCall | qualifiedName \u0026#39;(\u0026#39; (setQuantifier? expression (\u0026#39;,\u0026#39; expression)*)? (ORDER BY sortItem (\u0026#39;,\u0026#39; sortItem)*)? \u0026#39;)\u0026#39; filter? (nullTreatment? over)? #functionCall | identifier \u0026#39;-\u0026gt;\u0026#39; expression #lambda | \u0026#39;(\u0026#39; (identifier (\u0026#39;,\u0026#39; identifier)*)? \u0026#39;)\u0026#39; \u0026#39;-\u0026gt;\u0026#39; expression #lambda | \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #subqueryExpression // This is an extension to ANSI SQL, which considers EXISTS to be a \u0026lt;boolean expression\u0026gt; | EXISTS \u0026#39;(\u0026#39; query \u0026#39;)\u0026#39; #exists | CASE operand=expression whenClause+ (ELSE elseExpression=expression)? END #simpleCase | CASE whenClause+ (ELSE elseExpression=expression)? END #searchedCase | CAST \u0026#39;(\u0026#39; expression AS type \u0026#39;)\u0026#39; #cast | TRY_CAST \u0026#39;(\u0026#39; expression AS type \u0026#39;)\u0026#39; #cast | ARRAY \u0026#39;[\u0026#39; (expression (\u0026#39;,\u0026#39; expression)*)? \u0026#39;]\u0026#39; #arrayConstructor | value=primaryExpression \u0026#39;[\u0026#39; index=valueExpression \u0026#39;]\u0026#39; #subscript | identifier #columnReference | base=primaryExpression \u0026#39;.\u0026#39; fieldName=identifier #dereference | name=CURRENT_DATE #specialDateTimeFunction | name=CURRENT_TIME (\u0026#39;(\u0026#39; precision=INTEGER_VALUE \u0026#39;)\u0026#39;)? #specialDateTimeFunction | name=CURRENT_TIMESTAMP (\u0026#39;(\u0026#39; precision=INTEGER_VALUE \u0026#39;)\u0026#39;)? #specialDateTimeFunction | name=LOCALTIME (\u0026#39;(\u0026#39; precision=INTEGER_VALUE \u0026#39;)\u0026#39;)? #specialDateTimeFunction | name=LOCALTIMESTAMP (\u0026#39;(\u0026#39; precision=INTEGER_VALUE \u0026#39;)\u0026#39;)? #specialDateTimeFunction | name=CURRENT_USER #currentUser | name=CURRENT_PATH #currentPath | SUBSTRING \u0026#39;(\u0026#39; valueExpression FROM valueExpression (FOR valueExpression)? \u0026#39;)\u0026#39; #substring | NORMALIZE \u0026#39;(\u0026#39; valueExpression (\u0026#39;,\u0026#39; normalForm)? \u0026#39;)\u0026#39; #normalize | EXTRACT \u0026#39;(\u0026#39; identifier FROM valueExpression \u0026#39;)\u0026#39; #extract | \u0026#39;(\u0026#39; expression \u0026#39;)\u0026#39; #parenthesizedExpression | GROUPING \u0026#39;(\u0026#39; (qualifiedName (\u0026#39;,\u0026#39; qualifiedName)*)? \u0026#39;)\u0026#39; #groupingOperation ; nullTreatment : IGNORE NULLS | RESPECT NULLS ; string : STRING #basicStringLiteral | UNICODE_STRING (UESCAPE STRING)? #unicodeStringLiteral ; timeZoneSpecifier : TIME ZONE interval #timeZoneInterval | TIME ZONE string #timeZoneString ; comparisonOperator : EQ | NEQ | LT | LTE | GT | GTE ; comparisonQuantifier : ALL | SOME | ANY ; booleanValue : TRUE | FALSE ; interval : INTERVAL sign=(PLUS | MINUS)? string from=intervalField (TO to=intervalField)? ; intervalField : YEAR | MONTH | DAY | HOUR | MINUTE | SECOND ; normalForm : NFD | NFC | NFKD | NFKC ; type : ROW \u0026#39;(\u0026#39; rowField (\u0026#39;,\u0026#39; rowField)* \u0026#39;)\u0026#39; #rowType | INTERVAL from=intervalField (TO to=intervalField)? #intervalType | base=TIMESTAMP (\u0026#39;(\u0026#39; precision = INTEGER_VALUE \u0026#39;)\u0026#39;)? (WITHOUT TIME ZONE)? #dateTimeType | base=TIMESTAMP (\u0026#39;(\u0026#39; precision = INTEGER_VALUE \u0026#39;)\u0026#39;)? WITH TIME ZONE #dateTimeType | base=TIME (\u0026#39;(\u0026#39; precision = INTEGER_VALUE \u0026#39;)\u0026#39;)? (WITHOUT TIME ZONE)? #dateTimeType | base=TIME (\u0026#39;(\u0026#39; precision = INTEGER_VALUE \u0026#39;)\u0026#39;)? WITH TIME ZONE #dateTimeType | DOUBLE PRECISION #doublePrecisionType | ARRAY \u0026#39;\u0026lt;\u0026#39; type \u0026#39;\u0026gt;\u0026#39; #legacyArrayType | MAP \u0026#39;\u0026lt;\u0026#39; keyType=type \u0026#39;,\u0026#39; valueType=type \u0026#39;\u0026gt;\u0026#39; #legacyMapType | type ARRAY (\u0026#39;[\u0026#39; INTEGER_VALUE \u0026#39;]\u0026#39;)? #arrayType | identifier (\u0026#39;(\u0026#39; typeParameter (\u0026#39;,\u0026#39; typeParameter)* \u0026#39;)\u0026#39;)? #genericType ; rowField : identifier? type; typeParameter : INTEGER_VALUE | type ; whenClause : WHEN condition=expression THEN result=expression ; filter : FILTER \u0026#39;(\u0026#39; WHERE booleanExpression \u0026#39;)\u0026#39; ; over : OVER \u0026#39;(\u0026#39; (PARTITION BY partition+=expression (\u0026#39;,\u0026#39; partition+=expression)*)? (ORDER BY sortItem (\u0026#39;,\u0026#39; sortItem)*)? windowFrame? \u0026#39;)\u0026#39; ; windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ; frameBound : UNBOUNDED boundType=PRECEDING #unboundedFrame | UNBOUNDED boundType=FOLLOWING #unboundedFrame | CURRENT ROW #currentRowBound | expression boundType=(PRECEDING | FOLLOWING) #boundedFrame ; explainOption : FORMAT value=(TEXT | GRAPHVIZ | JSON) #explainFormat | TYPE value=(LOGICAL | DISTRIBUTED | VALIDATE | IO) #explainType ; qualifiedName : identifier (\u0026#39;.\u0026#39; identifier)* ; identifier : IDENTIFIER #unquotedIdentifier | QUOTED_IDENTIFIER #quotedIdentifier | nonReserved #unquotedIdentifier | BACKQUOTED_IDENTIFIER #backQuotedIdentifier | DIGIT_IDENTIFIER #digitIdentifier ; number : MINUS? DECIMAL_VALUE #decimalLiteral | MINUS? DOUBLE_VALUE #doubleLiteral | MINUS? INTEGER_VALUE #integerLiteral ; nonReserved // IMPORTANT: this rule must only contain tokens. Nested rules are not supported. See SqlParser.exitNonReserved : ADD | ADMIN | ALL | ANALYZE | ANY | ARRAY | ASC | AT | BERNOULLI | CALL | CASCADE | CATALOGS | COLUMN | COLUMNS | COMMENT | COMMIT | COMMITTED | CURRENT | DATA | DATE | DAY | DEFINER | DESC | DISTRIBUTED | DOUBLE | EXCLUDING | EXPLAIN | FETCH | FILTER | FIRST | FOLLOWING | FORMAT | FUNCTIONS | GRANT | GRANTED | GRANTS | GRAPHVIZ | HOUR | IF | IGNORE | INCLUDING | INPUT | INTERVAL | INVOKER | IO | ISOLATION | JSON | LAST | LATERAL | LEVEL | LIMIT | LOGICAL | MAP | MINUTE | MONTH | NEXT | NFC | NFD | NFKC | NFKD | NO | NONE | NULLIF | NULLS | OFFSET | ONLY | OPTION | ORDINALITY | OUTPUT | OVER | PARTITION | PARTITIONS | PATH | POSITION | PRECEDING | PRECISION | PRIVILEGES | PROPERTIES | RANGE | READ | RENAME | REPEATABLE | REPLACE | RESET | RESPECT | RESTRICT | REVOKE | ROLE | ROLES | ROLLBACK | ROW | ROWS | SCHEMA | SCHEMAS | SECOND | SECURITY | SERIALIZABLE | SESSION | SET | SETS | SHOW | SOME | START | STATS | SUBSTRING | SYSTEM | TABLES | TABLESAMPLE | TEXT | TIES | TIME | TIMESTAMP | TO | TRANSACTION | TRY_CAST | TYPE | UNBOUNDED | UNCOMMITTED | USE | USER | VALIDATE | VERBOSE | VIEW | WITHOUT | WORK | WRITE | YEAR | ZONE ; ADD: \u0026#39;ADD\u0026#39;; ADMIN: \u0026#39;ADMIN\u0026#39;; ALL: \u0026#39;ALL\u0026#39;; ALTER: \u0026#39;ALTER\u0026#39;; ANALYZE: \u0026#39;ANALYZE\u0026#39;; AND: \u0026#39;AND\u0026#39;; ANY: \u0026#39;ANY\u0026#39;; ARRAY: \u0026#39;ARRAY\u0026#39;; AS: \u0026#39;AS\u0026#39;; ASC: \u0026#39;ASC\u0026#39;; AT: \u0026#39;AT\u0026#39;; BERNOULLI: \u0026#39;BERNOULLI\u0026#39;; BETWEEN: \u0026#39;BETWEEN\u0026#39;; BY: \u0026#39;BY\u0026#39;; CALL: \u0026#39;CALL\u0026#39;; CASCADE: \u0026#39;CASCADE\u0026#39;; CASE: \u0026#39;CASE\u0026#39;; CAST: \u0026#39;CAST\u0026#39;; CATALOGS: \u0026#39;CATALOGS\u0026#39;; COLUMN: \u0026#39;COLUMN\u0026#39;; COLUMNS: \u0026#39;COLUMNS\u0026#39;; COMMENT: \u0026#39;COMMENT\u0026#39;; COMMIT: \u0026#39;COMMIT\u0026#39;; COMMITTED: \u0026#39;COMMITTED\u0026#39;; CONSTRAINT: \u0026#39;CONSTRAINT\u0026#39;; CREATE: \u0026#39;CREATE\u0026#39;; CROSS: \u0026#39;CROSS\u0026#39;; CUBE: \u0026#39;CUBE\u0026#39;; CURRENT: \u0026#39;CURRENT\u0026#39;; CURRENT_DATE: \u0026#39;CURRENT_DATE\u0026#39;; CURRENT_PATH: \u0026#39;CURRENT_PATH\u0026#39;; CURRENT_ROLE: \u0026#39;CURRENT_ROLE\u0026#39;; CURRENT_TIME: \u0026#39;CURRENT_TIME\u0026#39;; CURRENT_TIMESTAMP: \u0026#39;CURRENT_TIMESTAMP\u0026#39;; CURRENT_USER: \u0026#39;CURRENT_USER\u0026#39;; DATA: \u0026#39;DATA\u0026#39;; DATE: \u0026#39;DATE\u0026#39;; DAY: \u0026#39;DAY\u0026#39;; DEALLOCATE: \u0026#39;DEALLOCATE\u0026#39;; DEFINER: \u0026#39;DEFINER\u0026#39;; DELETE: \u0026#39;DELETE\u0026#39;; DESC: \u0026#39;DESC\u0026#39;; DESCRIBE: \u0026#39;DESCRIBE\u0026#39;; DISTINCT: \u0026#39;DISTINCT\u0026#39;; DISTRIBUTED: \u0026#39;DISTRIBUTED\u0026#39;; DOUBLE: \u0026#39;DOUBLE\u0026#39;; DROP: \u0026#39;DROP\u0026#39;; ELSE: \u0026#39;ELSE\u0026#39;; END: \u0026#39;END\u0026#39;; ESCAPE: \u0026#39;ESCAPE\u0026#39;; EXCEPT: \u0026#39;EXCEPT\u0026#39;; EXCLUDING: \u0026#39;EXCLUDING\u0026#39;; EXECUTE: \u0026#39;EXECUTE\u0026#39;; EXISTS: \u0026#39;EXISTS\u0026#39;; EXPLAIN: \u0026#39;EXPLAIN\u0026#39;; EXTRACT: \u0026#39;EXTRACT\u0026#39;; FALSE: \u0026#39;FALSE\u0026#39;; FETCH: \u0026#39;FETCH\u0026#39;; FILTER: \u0026#39;FILTER\u0026#39;; FIRST: \u0026#39;FIRST\u0026#39;; FOLLOWING: \u0026#39;FOLLOWING\u0026#39;; FOR: \u0026#39;FOR\u0026#39;; FORMAT: \u0026#39;FORMAT\u0026#39;; FROM: \u0026#39;FROM\u0026#39;; FULL: \u0026#39;FULL\u0026#39;; FUNCTIONS: \u0026#39;FUNCTIONS\u0026#39;; GRANT: \u0026#39;GRANT\u0026#39;; GRANTED: \u0026#39;GRANTED\u0026#39;; GRANTS: \u0026#39;GRANTS\u0026#39;; GRAPHVIZ: \u0026#39;GRAPHVIZ\u0026#39;; GROUP: \u0026#39;GROUP\u0026#39;; GROUPING: \u0026#39;GROUPING\u0026#39;; HAVING: \u0026#39;HAVING\u0026#39;; HOUR: \u0026#39;HOUR\u0026#39;; IF: \u0026#39;IF\u0026#39;; IGNORE: \u0026#39;IGNORE\u0026#39;; IN: \u0026#39;IN\u0026#39;; INCLUDING: \u0026#39;INCLUDING\u0026#39;; INNER: \u0026#39;INNER\u0026#39;; INPUT: \u0026#39;INPUT\u0026#39;; INSERT: \u0026#39;INSERT\u0026#39;; INTERSECT: \u0026#39;INTERSECT\u0026#39;; INTERVAL: \u0026#39;INTERVAL\u0026#39;; INTO: \u0026#39;INTO\u0026#39;; INVOKER: \u0026#39;INVOKER\u0026#39;; IO: \u0026#39;IO\u0026#39;; IS: \u0026#39;IS\u0026#39;; ISOLATION: \u0026#39;ISOLATION\u0026#39;; JSON: \u0026#39;JSON\u0026#39;; JOIN: \u0026#39;JOIN\u0026#39;; LAST: \u0026#39;LAST\u0026#39;; LATERAL: \u0026#39;LATERAL\u0026#39;; LEFT: \u0026#39;LEFT\u0026#39;; LEVEL: \u0026#39;LEVEL\u0026#39;; LIKE: \u0026#39;LIKE\u0026#39;; LIMIT: \u0026#39;LIMIT\u0026#39;; LOCALTIME: \u0026#39;LOCALTIME\u0026#39;; LOCALTIMESTAMP: \u0026#39;LOCALTIMESTAMP\u0026#39;; LOGICAL: \u0026#39;LOGICAL\u0026#39;; MAP: \u0026#39;MAP\u0026#39;; MINUTE: \u0026#39;MINUTE\u0026#39;; MONTH: \u0026#39;MONTH\u0026#39;; NATURAL: \u0026#39;NATURAL\u0026#39;; NEXT: \u0026#39;NEXT\u0026#39;; NFC : \u0026#39;NFC\u0026#39;; NFD : \u0026#39;NFD\u0026#39;; NFKC : \u0026#39;NFKC\u0026#39;; NFKD : \u0026#39;NFKD\u0026#39;; NO: \u0026#39;NO\u0026#39;; NONE: \u0026#39;NONE\u0026#39;; NORMALIZE: \u0026#39;NORMALIZE\u0026#39;; NOT: \u0026#39;NOT\u0026#39;; NULL: \u0026#39;NULL\u0026#39;; NULLIF: \u0026#39;NULLIF\u0026#39;; NULLS: \u0026#39;NULLS\u0026#39;; OFFSET: \u0026#39;OFFSET\u0026#39;; ON: \u0026#39;ON\u0026#39;; ONLY: \u0026#39;ONLY\u0026#39;; OPTION: \u0026#39;OPTION\u0026#39;; OR: \u0026#39;OR\u0026#39;; ORDER: \u0026#39;ORDER\u0026#39;; ORDINALITY: \u0026#39;ORDINALITY\u0026#39;; OUTER: \u0026#39;OUTER\u0026#39;; OUTPUT: \u0026#39;OUTPUT\u0026#39;; OVER: \u0026#39;OVER\u0026#39;; PARTITION: \u0026#39;PARTITION\u0026#39;; PARTITIONS: \u0026#39;PARTITIONS\u0026#39;; PATH: \u0026#39;PATH\u0026#39;; POSITION: \u0026#39;POSITION\u0026#39;; PRECEDING: \u0026#39;PRECEDING\u0026#39;; PREPARE: \u0026#39;PREPARE\u0026#39;; PRIVILEGES: \u0026#39;PRIVILEGES\u0026#39;; PRECISION: \u0026#39;PRECISION\u0026#39;; PROPERTIES: \u0026#39;PROPERTIES\u0026#39;; RANGE: \u0026#39;RANGE\u0026#39;; READ: \u0026#39;READ\u0026#39;; RECURSIVE: \u0026#39;RECURSIVE\u0026#39;; RENAME: \u0026#39;RENAME\u0026#39;; REPEATABLE: \u0026#39;REPEATABLE\u0026#39;; REPLACE: \u0026#39;REPLACE\u0026#39;; RESET: \u0026#39;RESET\u0026#39;; RESPECT: \u0026#39;RESPECT\u0026#39;; RESTRICT: \u0026#39;RESTRICT\u0026#39;; REVOKE: \u0026#39;REVOKE\u0026#39;; RIGHT: \u0026#39;RIGHT\u0026#39;; ROLE: \u0026#39;ROLE\u0026#39;; ROLES: \u0026#39;ROLES\u0026#39;; ROLLBACK: \u0026#39;ROLLBACK\u0026#39;; ROLLUP: \u0026#39;ROLLUP\u0026#39;; ROW: \u0026#39;ROW\u0026#39;; ROWS: \u0026#39;ROWS\u0026#39;; SCHEMA: \u0026#39;SCHEMA\u0026#39;; SCHEMAS: \u0026#39;SCHEMAS\u0026#39;; SECOND: \u0026#39;SECOND\u0026#39;; SECURITY: \u0026#39;SECURITY\u0026#39;; SELECT: \u0026#39;SELECT\u0026#39;; SERIALIZABLE: \u0026#39;SERIALIZABLE\u0026#39;; SESSION: \u0026#39;SESSION\u0026#39;; SET: \u0026#39;SET\u0026#39;; SETS: \u0026#39;SETS\u0026#39;; SHOW: \u0026#39;SHOW\u0026#39;; SOME: \u0026#39;SOME\u0026#39;; START: \u0026#39;START\u0026#39;; STATS: \u0026#39;STATS\u0026#39;; SUBSTRING: \u0026#39;SUBSTRING\u0026#39;; SYSTEM: \u0026#39;SYSTEM\u0026#39;; TABLE: \u0026#39;TABLE\u0026#39;; TABLES: \u0026#39;TABLES\u0026#39;; TABLESAMPLE: \u0026#39;TABLESAMPLE\u0026#39;; TEXT: \u0026#39;TEXT\u0026#39;; THEN: \u0026#39;THEN\u0026#39;; TIES: \u0026#39;TIES\u0026#39;; TIME: \u0026#39;TIME\u0026#39;; TIMESTAMP: \u0026#39;TIMESTAMP\u0026#39;; TO: \u0026#39;TO\u0026#39;; TRANSACTION: \u0026#39;TRANSACTION\u0026#39;; TRUE: \u0026#39;TRUE\u0026#39;; TRY_CAST: \u0026#39;TRY_CAST\u0026#39;; TYPE: \u0026#39;TYPE\u0026#39;; UESCAPE: \u0026#39;UESCAPE\u0026#39;; UNBOUNDED: \u0026#39;UNBOUNDED\u0026#39;; UNCOMMITTED: \u0026#39;UNCOMMITTED\u0026#39;; UNION: \u0026#39;UNION\u0026#39;; UNNEST: \u0026#39;UNNEST\u0026#39;; USE: \u0026#39;USE\u0026#39;; USER: \u0026#39;USER\u0026#39;; USING: \u0026#39;USING\u0026#39;; VALIDATE: \u0026#39;VALIDATE\u0026#39;; VALUES: \u0026#39;VALUES\u0026#39;; VERBOSE: \u0026#39;VERBOSE\u0026#39;; VIEW: \u0026#39;VIEW\u0026#39;; WHEN: \u0026#39;WHEN\u0026#39;; WHERE: \u0026#39;WHERE\u0026#39;; WITH: \u0026#39;WITH\u0026#39;; WITHOUT: \u0026#39;WITHOUT\u0026#39;; WORK: \u0026#39;WORK\u0026#39;; WRITE: \u0026#39;WRITE\u0026#39;; YEAR: \u0026#39;YEAR\u0026#39;; ZONE: \u0026#39;ZONE\u0026#39;; EQ : \u0026#39;=\u0026#39;; NEQ : \u0026#39;\u0026lt;\u0026gt;\u0026#39; | \u0026#39;!=\u0026#39;; LT : \u0026#39;\u0026lt;\u0026#39;; LTE : \u0026#39;\u0026lt;=\u0026#39;; GT : \u0026#39;\u0026gt;\u0026#39;; GTE : \u0026#39;\u0026gt;=\u0026#39;; PLUS: \u0026#39;+\u0026#39;; MINUS: \u0026#39;-\u0026#39;; ASTERISK: \u0026#39;*\u0026#39;; SLASH: \u0026#39;/\u0026#39;; PERCENT: \u0026#39;%\u0026#39;; CONCAT: \u0026#39;||\u0026#39;; STRING : \u0026#39;\\\u0026#39;\u0026#39; ( ~\u0026#39;\\\u0026#39;\u0026#39; | \u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39; )* \u0026#39;\\\u0026#39;\u0026#39; ; UNICODE_STRING : \u0026#39;U\u0026amp;\\\u0026#39;\u0026#39; ( ~\u0026#39;\\\u0026#39;\u0026#39; | \u0026#39;\\\u0026#39;\\\u0026#39;\u0026#39; )* \u0026#39;\\\u0026#39;\u0026#39; ; // Note: we allow any character inside the binary literal and validate // its a correct literal when the AST is being constructed. This // allows us to provide more meaningful error messages to the user BINARY_LITERAL : \u0026#39;X\\\u0026#39;\u0026#39; (~\u0026#39;\\\u0026#39;\u0026#39;)* \u0026#39;\\\u0026#39;\u0026#39; ; INTEGER_VALUE : DIGIT+ ; DECIMAL_VALUE : DIGIT+ \u0026#39;.\u0026#39; DIGIT* | \u0026#39;.\u0026#39; DIGIT+ ; DOUBLE_VALUE : DIGIT+ (\u0026#39;.\u0026#39; DIGIT*)? EXPONENT | \u0026#39;.\u0026#39; DIGIT+ EXPONENT ; IDENTIFIER : (LETTER | \u0026#39;_\u0026#39;) (LETTER | DIGIT | \u0026#39;_\u0026#39; | \u0026#39;@\u0026#39; | \u0026#39;:\u0026#39;)* ; DIGIT_IDENTIFIER : DIGIT (LETTER | DIGIT | \u0026#39;_\u0026#39; | \u0026#39;@\u0026#39; | \u0026#39;:\u0026#39;)+ ; QUOTED_IDENTIFIER : \u0026#39;\u0026#34;\u0026#39; ( ~\u0026#39;\u0026#34;\u0026#39; | \u0026#39;\u0026#34;\u0026#34;\u0026#39; )* \u0026#39;\u0026#34;\u0026#39; ; BACKQUOTED_IDENTIFIER : \u0026#39;`\u0026#39; ( ~\u0026#39;`\u0026#39; | \u0026#39;``\u0026#39; )* \u0026#39;`\u0026#39; ; fragment EXPONENT : \u0026#39;E\u0026#39; [+-]? DIGIT+ ; fragment DIGIT : [0-9] ; fragment LETTER : [A-Z] ; SIMPLE_COMMENT : \u0026#39;--\u0026#39; ~[\\r\\n]* \u0026#39;\\r\u0026#39;? \u0026#39;\\n\u0026#39;? -\u0026gt; channel(HIDDEN) ; BRACKETED_COMMENT : \u0026#39;/*\u0026#39; .*? \u0026#39;*/\u0026#39; -\u0026gt; channel(HIDDEN) ; WS : [ \\r\\n\\t]+ -\u0026gt; channel(HIDDEN) ; // Catch-all for anything we can\u0026#39;t recognize. // We use this to be able to ignore and recover all the text // when splitting statements with DelimiterLexer UNRECOGNIZED : . ; "},{"url":"/docs/use-exisitng-data/tables-in-a-bucket/doc/","title":"Tables-in-a-bucket","content":"What is a phenopacket? Phenopacket is a GA4GH approved standard file format for sharing phenotypic information.\nFind documentation for phenopackets-schema here.\nWhat does it contain?  A set of mandatory and optional fields to share information about a patient or participant’s phenotype Optional fields may include clinical diagnosis, age of onset, results from lab tests, and disease severity.  An example of the JSON structure can be found here.\nPreparing the data into a GA4GH Search application If you need some phenopackets data to follow this example, consider the following:\n Human Phenotype Ontology phenopackets downloaded from a publicly available metadata source: https://zenodo.org/record/3905420#.X3Sd2pNKj6h. Gecco Biosample phenopackets.   Clone this repository: https://github.com/DNAstack/tables-loader All phenopacket JSON files should be added under \u0026lt;repo_root\u0026gt;/resources/phenopackets Create a Google storage bucket with public access. Creating Storage Buckets Create a service account and grant \u0026ldquo;Storage Admin\u0026rdquo; access to your storage bucket. You should automatically begin downloading the access key. *Store this access key somewhere safe, you\u0026rsquo;ll need it later  Running the Spring application  Export this environment variable: GOOGLE_APPLICATION_CREDENTIALS=\u0026lt;service_account_json_file_path\u0026gt; Start the Spring application. (With your Java IDE or mvn clean spring-boot:run in bash) You should see the following at http://localhost:8080/: Welcome to Phenopackets tables loader application!  Available API endpoints /create-tables-files/{tableStructure}: Creates tables, info and data files for all phenopackets JSON present in \u0026lt;repo_root\u0026gt;/resources/phenopackets directory\n/upload-files/{tableStructure} Uploads all files under \u0026lt;repo_root\u0026gt;/resources/ga4gh-phenopackets-example/\u0026lt;by_subject|flat\u0026gt; to Google cloud storage bucket ga4gh-phenopackets-example\nphenopacket/{tableStructure}/tables Gets \u0026lt;repo_root\u0026gt;/resources/ga4gh-phenopackets-example/\u0026lt;by_subject|flat\u0026gt;/tables json file\nphenopacket/{tableStructure}/table/{tableName}/info Gets \u0026lt;repo_root\u0026gt;/resources/ga4gh-phenopackets-example/\u0026lt;by_subject|flat\u0026gt;/table/info file\nphenopacket/{tableStructure}/table/{tableName}/data Gets \u0026lt;repo_root\u0026gt;/resources/ga4gh-phenopackets-example/\u0026lt;by_subject|flat\u0026gt;/table/data file\n"},{"url":"/docs/use-exisitng-data/using-preso/doc/","title":"Using Presto","content":"The dbGaP GECCO example In the provision data section, we\u0026rsquo;ve shown a quick start recipe with the ga4gh-search-adapter-presto docker container connected to a Presto instance hosted at https://presto-public.prod.dnastack.com. This section provides more information on how this was accomplished.\nQuick Links  ga4gh-search-adapter-presto\nOpen API 3 Reference\nFull GA4GH Search Specification\nTable Object Specification\nSearch API’s SQL dialect\n  Prerequisites The following is required before we start.\n Java 11+ A Presto server you can access anonymously over HTTP(S). Git   If you don\u0026rsquo;t have a Presto server to work against and you wish to try the app, try using https://presto-public.prod.dnastack.com as the data source.\n 1. Building the Presto Adapter App\nClone the repository\ngit clone https://github.com/DNAstack/ga4gh-search-adapter-presto.git Build the app\nmvn clean package 2. Configuration\nFor a minimal configuration, we need to provide two parameters, PRESTO_DATASOURCE_URL and SPRING_PROFILES_ACTIVE.\nPRESTO_DATASOURCE_URL points to the Presto server you wish to expose with a Search API.\nClone the repository:\nexport PRESTO_DATASOURCE_URL=https://\u0026lt;your-presto-server\u0026gt; export SPRING_PROFILES_ACTIVE=no-auth The adapter app requires a local PostgreSQL database connection. To start the app locally with the default settings, you can spin up the database with this docker command:\ndocker run -d -p 5432:5432 --name ga4ghsearchadapterpresto -e POSTGRES_USER=ga4ghsearchadapterpresto -e POSTGRES_PASSWORD=ga4ghsearchadapterpresto postgres 3. Run the adapter app\nmvn clean spring-boot:run Your application should now be accessible at http://localhost:8089/tables\nTo test the app out, follow the consuming data section.\nFurther Configuration Further configuration can be found at: https://github.com/DNAstack/ga4gh-search-adapter-presto\n"},{"url":"/docs/getting-started/provision-data/","title":"Provision Data","content":"{row-divider}\nImplementation The GA4GH API requires table operations to be implemented to specification for basic discovery and browsing.\nOptional but not required, query operations may be implemented to support querying with SQL.\nThe Search API is backend agnostic, which means any solution that implements the API specification is valid. You can use your favorite REST application frameworks to implement GA4GH Search Endpoints or a hosted blob store for a tables-in-a-bucket implementation requiring no code.\nCheckout the following examples for some inspiration. {divider} Quick Links  Full API Specifications\nPlaceholder for custodian examples\n  {row-divider}\nTables-in-a-bucket example The specification allows for a no-code implementation as a collection of files served statically. This is the easiest way to start experimenting with the GA4GH Search API. As long as your storage bucket conforms to the correct file structure and it has the correct sharing permissions, it is a valid Search implementation.\nA concrete example implementation is available here and try browsing this implementation with these commands.\n{divider} Here\u0026rsquo;s how you\u0026rsquo;ll need to organize your folders\n tables: served in response to GET /tables table/{table_name}/info: served in response to GET /table/{table_name}/info. e.g. a table with the name mytable should have a corresponding file table/mytable/info table/{table_name}/data: served in response to GET /table/{table_name}/data. e.g. a table with the name mytable should have a corresponding file table/mytable/data table/{table_name}/data_{pageNumber}, which will be linked in the next_page_url of the first table (e.g. mytable). table/{table_name}/data_models/{schemaFile}: Though not required, data models may be linked via $ref. Data models can also be stored as static JSON documents, and be referred to by relative or absolute URLs.   \n{row-divider}\nTry out a reference implementation This example was shown as a demo during the 2020 GA4GH Plenary. This app will run a reference Search implementation on docker and use a Presto instance hosted by DNAstack as the data source.\nYou’ll need docker set up on your system to run the Spring app, and you’ll need to have one of the client libraries installed from the Introduction Section.\nFurther information about this example can be found here. {divider}  30 second quick start   docker pull postgres:latest docker run -d --rm --network=\u0026#34;host\u0026#34; --name dnastack-ga4gh-search-db -e POSTGRES_USER=ga4ghsearchadapterpresto -e POSTGRES_PASSWORD=ga4ghsearchadapterpresto postgres docker pull dnastack/ga4gh-search-adapter-presto:latest docker run --rm --name dnastack-ga4gh-search -p 8089:8089 -e PRESTO_DATASOURCE_URL=https://presto-public.prod.dnastack.com -e SPRING_PROFILES_ACTIVE=no-auth dnastack/ga4gh-search-adapter-presto:latest    \n Python R CLI   # init search client from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;http://localhost:8089/\u0026#39; search_client = SearchClient(base_url=base_url) # get tables tables_iterator = search_client.get_table_list() tables = [next(tables_iterator, None) for i in range(10)] tables = list(filter(None, tables)) print(tables) # get table info table_name = \u0026#34;sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34; table_info = search_client.get_table_info(table_name) print(table_info) # get table data table_name = \u0026#34;sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34; table_data_iterator = search_client.get_table_data(table_name) table_data = [next(table_data_iterator, None) for i in range(10)] table_data = list(filter(None, table_data)) print(table_data)   # Fetch table list library(httr) tables \u0026lt;- ga4gh.search::ga4gh_list_tables(\u0026#34;http://localhost:8089\u0026#34;) print(tables) # Try a query search_result \u0026lt;- ga4gh.search::ga4gh_search(\u0026#34;http://localhost:8089\u0026#34;, \u0026#34;SELECT sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34;) print(tables)   List tables\nsearch-cli list --api-url http://localhost:8089 Get table info\nsearch-cli info dbgap_demo.scr_gecco_susceptibility.sample_multi --api-url http://localhost:8089 Get table data\nsearch-cli data dbgap_demo.scr_gecco_susceptibility.sample_multi --api-url http://localhost:8089     "},{"url":"/docs/getting-started/consume-data/","title":"Consume Data","content":"{row-divider}\nBrowsing The minimum Search API implementations will support browsing by table. This means these operations from the API specs are supported for table by table browsing.\nOn the right is example code to browse the tables-in-a-bucket implementation of Search. {divider}  Python R CLI   Follow along in Colab\n# init search client from search_python_client.search import DrsClient, SearchClient base_url_tiab = \u0026#39;https://storage.googleapis.com/ga4gh-tables-example/\u0026#39; search_client_tiab = SearchClient(base_url=base_url_tiab) # get tables tables_iterator = search_client_tiab.get_table_list() tables = [next(tables_iterator, None) for i in range(10)] tables = list(filter(None, tables)) print(tables) # get table info table_name = tables[0][\u0026#39;name\u0026#39;] table_info = search_client_tiab.get_table_info(table_name) print(table_info) # get table data table_name = tables[0][\u0026#39;name\u0026#39;] table_data_iterator = search_client_tiab.get_table_data(table_name) table_data = [next(table_data_iterator, None) for i in range(10)] table_data = list(filter(None, table_data)) print(table_data)   Under construction https://colab.research.google.com/drive/1VOP2IcPjsX4U-DfuiTs7Tr0SVlAD0IMh?usp=sharing \u0026lt;= doesn't work right now.   Get list of tables\nsearch-cli list --api-url https://storage.googleapis.com/ga4gh-tables-example search-cli info subjects --api-url https://storage.googleapis.com/ga4gh-tables-example search-cli data subjects --api-url https://storage.googleapis.com/ga4gh-tables-example   \n{row-divider}\nQueries The Search API supports query operation through SQL statements.\nThe GA4GH Search API\u0026rsquo;s SQL dialect has a familiar interface inspired by current major open source database platforms, including Presto SQL, PostgreSQL, MySQL, and BigQuery. If you have prior experience with these database platforms, you\u0026rsquo;ll feel right at home with only minor adjustments.\nSupported SQL functions\nSupported SQL grammar\n{divider}  Example #1 Example #2   This query returns all female patients from the patient table.\n/* you can scroll on this tab */ SELECT * FROM kidsfirst.ga4gh_tables.patient WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5;   This query returns all conditions observed in female patients from the patient table.\n/* you can scroll on this tab */ SELECT Json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) AS disease, Json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) AS identifier FROM kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id = REPLACE(Json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5;    \n{row-divider}\nIssuing queries using the Search API Search is a standard REST API. This means Search can be accessed through standard HTTP calls.\nWhile Search API can be navigated using programs like cURL or Postman, it is best accessed programmatically. The results could return multiple pages, which is easier to navigate with programmatic access.\nOnce you\u0026rsquo;ve visited a page and consumed data from it, you can\u0026rsquo;t go back to it or refresh.\nOn the right, we provide examples to consume data from the Search API using the GA4GH Commandline Interface, the R client, Python, and cURL.\n Need help installing client libraries?\n {divider}  Python R CLI cURL   Follow Along in Google Colab\n# Installing the client library form PyPi pip install search-python-client # Installing from Github pip install git+https://github.com/DNAstack/search-python-client --no-cache-dir # Building the query from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://search-presto-public.staging.dnastack.com\u0026#39; search_client = SearchClient(base_url=base_url) query = \u0026#34;\u0026#34;\u0026#34; SELECT Json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) AS disease, Json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) AS identifier FROM kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id = REPLACE(Json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5 \u0026#34;\u0026#34;\u0026#34; # Executing the query table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item) # Results {\u0026#39;disease\u0026#39;: \u0026#39;Aortic atresia\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Aortic atresia|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Mitral atresia\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Mitral atresia|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplasia ascending aorta\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplasia ascending aorta|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplastic left heart syndrome\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplastic left heart syndrome|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplastic left ventricle (subnormal cavity volume)\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplastic left ventricle (subnormal cavity volume)|None\u0026#39;}   Follow Along in Google Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) conditionsInFemalePatients \u0026lt;- ga4gh.search::ga4gh_search(\u0026#34;https://search-presto-public.staging.dnastack.com\u0026#34;, \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;)=\u0026#39;female\u0026#39; limit 5\u0026#34;) # View the results print(conditionsInFemalePatients) Output:\ndisease 1 Aortic atresia 2 Mitral atresia 3 Hypoplasia ascending aorta 4 Hypoplastic left heart syndrome 5 Hypoplastic left ventricle (subnormal cavity volume) identifier 1 Condition|SD_PREASA7S|272|Aortic atresia|None 2 Condition|SD_PREASA7S|272|Mitral atresia|None 3 Condition|SD_PREASA7S|272|Hypoplasia ascending aorta|None 4 Condition|SD_PREASA7S|272|Hypoplastic left heart syndrome|None 5 Condition|SD_PREASA7S|272|Hypoplastic left ventricle (subnormal cavity volume)|None   search-cli query -q \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;)=\u0026#39;female\u0026#39; limit 5\u0026#34; --api-url https://search-presto-public.staging.dnastack.com   These requests This query returns all female patients from the patient table.\ncurl --request POST \\  --url https://search-presto-public.staging.dnastack.com/search \\  --header \u0026#39;content-type: application/json\u0026#39; \\  --data \u0026#39;{ \u0026#34;query\u0026#34;: \u0026#34;select * from kidsfirst.ga4gh_tables.patient WHERE json_extract_scalar(patient, \u0026#39;\\\u0026#39;\u0026#39;$.gender\u0026#39;\\\u0026#39;\u0026#39;)=\u0026#39;\\\u0026#39;\u0026#39;female\u0026#39;\\\u0026#39;\u0026#39; limit 5\u0026#34;}\u0026#39; This query returns all conditions observed in female patients from the patient table.\ncurl --request POST \\  --url https://search-presto-public.staging.dnastack.com/search \\  --header \u0026#39;content-type: application/json\u0026#39; \\  --data \u0026#39;{ \u0026#34;query\u0026#34;: \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.code.text\u0026#39;\\\u0026#39;\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.identifier[0].value\u0026#39;\\\u0026#39;\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.subject.reference\u0026#39;\\\u0026#39;\u0026#39;), \u0026#39;\\\u0026#39;\u0026#39;Patient/\u0026#39;\\\u0026#39;\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;\\\u0026#39;\u0026#39;$.gender\u0026#39;\\\u0026#39;\u0026#39;)=\u0026#39;\\\u0026#39;\u0026#39;female\u0026#39;\\\u0026#39;\u0026#39; limit 5\u0026#34;}\u0026#39;    \n{row-divider}\nMore Examples dbGaP GECCO Example This is a public implementation of Search. Feel free to follow along with the examples and explore this endpoint with your own script.  Python R CLI   Follow along in Colab\n# init search client from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://search-presto-public.prod.dnastack.com/\u0026#39; search_client = SearchClient(base_url=base_url) # Find available tables tables_iterator = search_client.get_table_list() tables = list(tables_iterator) import pprint pprint.pprint(tables) #Get more information about a table returned table_info = search_client.get_table_info(\u0026#34;dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi\u0026#34;) pprint.pprint(table_info) # Dig into the table a little further table_data_iterator = search_client.get_table_data(\u0026#34;dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi\u0026#34;) # Limit to first 10 items tables = [next(table_data_iterator, None) for i in range(10)] tables = list(filter(None, tables)) pprint.pprint(tables) # Select all items from the CPS-II study  query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5 \u0026#34;\u0026#34;\u0026#34; # Executing the query table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item)   Follow along in Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) ga4gh.search::ga4gh_list_tables(\u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34;) # Select all items from the CPS-II study  query \u0026lt;- \u0026#34;SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5\u0026#34; # Executing the query ga4gh.search::ga4gh_search(\u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34;, query)   List tables\nsearch-cli list --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; Get table info\nsearch-cli info dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; Now run a query and pipe the results to a file called results.txt\nsearch-cli query -q \u0026#34;SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5\u0026#34; \\  --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; \u0026gt; results.txt   \n{divider}\n COVID Cloud Example This is a public implementation of Search. It is connected to multiple sources of data related to COVID19, such as data from USA Facts used in this example.  Python R CLI   Follow along in Colab\nfrom search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://search-presto-public-covid19.prod.dnastack.com/\u0026#39; search_client = SearchClient(base_url=base_url) # Find available tables tables_iterator = search_client.get_table_list() tables = list(tables_iterator) import pprint pprint.pprint(tables) #Get more information about a table returned table_info = search_client.get_table_info(\u0026#34;coronavirus_public.covid19_usafacts.deaths\u0026#34;) pprint.pprint(table_info) # Dig into the table a little further table_data_iterator = search_client.get_table_data(\u0026#34;coronavirus_public.covid19_usafacts.deaths\u0026#34;) # Limit to first 10 items tables = [next(table_data_iterator, None) for i in range(10)] tables = list(filter(None, tables)) pprint.pprint(tables) # Select all corona death cases from the state of LA, limited to 25 results and sorted by county name. query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM coronavirus_public.covid19_usafacts.deaths WHERE state = \u0026#39;LA\u0026#39; ORDER BY county_name LIMIT 25 \u0026#34;\u0026#34;\u0026#34; # Executing the query table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item)   Follow along in Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) ga4gh.search::ga4gh_list_tables(\u0026#34;https://search-presto-public-covid19.prod.dnastack.com\u0026#34;) # Select all COVID death cases from the state of LA, limited to 25 results and sorted by county name. query \u0026lt;- \u0026#34;SELECT * FROM coronavirus_public.covid19_usafacts.deaths WHERE state = \u0026#39;LA\u0026#39; ORDER BY county_name LIMIT 25\u0026#34; # Executing the query ga4gh.search::ga4gh_search(\u0026#34;https://search-presto-public-covid19.prod.dnastack.com\u0026#34;, query)   List tables\nsearch-cli list --api-url \u0026#34;https://search-presto-public-covid19.prod.dnastack.com\u0026#34; Get table info\nsearch-cli info coronavirus_public.covid19_usafacts.deaths --api-url \u0026#34;https://search-presto-public-covid19.prod.dnastack.com\u0026#34; Now run a query and pipe the results to a file called results.txt\nsearch-cli query -q \u0026#34;SELECT * FROM coronavirus_public.covid19_usafacts.deaths WHERE state = \u0026#39;LA\u0026#39; ORDER BY county_name LIMIT 25\u0026#34; \\  --api-url \u0026#34;https://search-presto-public-covid19.prod.dnastack.com\u0026#34; \u0026gt; results.txt   \n"},{"url":"/docs/getting-started/upload-result/","title":"Upload results","content":"{row-divider}\nWe\u0026rsquo;re working on it!\n"},{"url":"/docs/index.json","title":"Docs","content":""}]