[{"url":"/docs/security/data-source/","title":"At Data Sources","content":"Securing Data Sources This page discusses different approaches to securing a Data Connect implementation depending on which implementation path you choose and how complex your access needs are.\nData Connect can be implemented in many ways. For example:\n as static files in a web server or cloud bucket (\u0026ldquo;tables in a bucket\u0026rdquo;) in front of a single database server (for example, PostgreSQL, MySQL, or ElasticSearch) in front of many database servers (for example, using Trino)  In addition, your dataset might require a single tier of access, where someone either has access to the whole thing or nothing at all, or you might require multiple access tiers where different users can access different subsets of the data.\nTables in a Bucket If you implement Data Connect using static JSON files in a web server or cloud file storage system, you can set the web server or cloud bucket to require an authentication tokens with each request.\nIf you are hosting your tables in a web server such as nginx, Express.js, Tomcat, or Apache HTTPD, you have the option of providing a custom authentication module that understands JWT bearer tokens.\nWith data consumers accessing cloud buckets directly, the easiest approach is to use an authentication mechanism supported by the cloud vendor. This may be acceptable if the data consumers are inside your own organization, and they already have a way to obtain cloud credentials.\nTo customize the authentication mechanism on tables-in-a-bucket (for example, if you are experimenting with a GA4GH Passport integration) then you may have a few options, depending on the cloud platform:\n Put your cloud bucket behind an HTTP proxy that checks authentication in a custom way. If you do this, ensure links such as nextPageUrl are relative in all your JSON files. Check if your cloud storage system can delegate request authorization to a serverless function that you supply (eg. AWS Lambda, Google Cloud Function, Azure Function). This may be possible directly, or you may need to route requests through an API Gateway.  Multi Tiered Access With tables-in-a-bucket, consider creating separate Data Connect implementations, each in their own bucket, for each access tier. This allows you to keep access policies uniform with each bucket, and gives you the flexibility to provide different data granularity to users within each tier.\nIn Front of a Single Database In this case, you will be running custom server code that translates incoming requests from Data Connect API requests into the format natively understood by your backend database.\nSingle Tiered Access Create a single database user for your Data Connect API server. Grant this user read-only access to only the tables that you wish to expose via Data Connect. Your custom server will access the database as this user.\nOn each incoming Data Connect HTTP request, check for a valid OAuth2 bearer token. If the token is valid, make the corresponding request to the backend database. The Data Connect API user\u0026rsquo;s scope of access will be limited to what the database user can see.\nMulti Tiered Access Create a database user for each access tier your Data Connect API server will support. Grant each user read-only access to only the tables that you wish to expose at that access tier. Your custom server will select the correct database user based on the credentials in the incoming requests.\nOn each incoming Data Connect HTTP request, check for a valid JWT OAuth2 bearer token. If the token is valid, examine its claims and select the appropriate database user. The Data Connect API user\u0026rsquo;s scope of access will be limited to what the database user for their access tier can see.\nIf some access tiers should only see data at a coarser grain (for example, cohort-level statistics rather than subject-level data), consider one of the following approaches:\n create views of the data that only reveal data at the coarser grain, and grant the \u0026lsquo;tiered database users\u0026rsquo; access to these views only pre-aggregate the data into tables or materialized views, and grant the \u0026lsquo;tiered database users\u0026rsquo; access to these tables or materialized views only  Since there will typically be many more users with access to the coarser-grained view of the data, pre-aggregating the data offers a performance advantage as well.\nIn Front of Many Databases If you are exposing many databases under a single Data Connect API instance, you are probably using a Trino based implementation.\nTrino provides the SystemAccessControl interface which you can implement yourself to secure your data source.\nA Trino-based Data Connect implementation will have a Data Connect API adapter service in front of Trino which accepts Data Connect API calls and relays them to Trino in its own API, just like the single database case outlined above. The adapter service should extract the user\u0026rsquo;s JWT bearer token from the inbound request and include it in the Trino request under the X-Trino-Extra-Credential header.\nFrom there, your implementation of the SystemAccessControl interface will have access to the JWT and its claims, and will be able to control access:\n Allow/Deny:  Access per catalog, schema, table, and column Access to see the definition of a view   Filter:  Visibility of catalogs, schemas, tables, and columns Row-by-row table data using a filter expression    "},{"url":"/docs/security/search-endpoint/","title":"At Endpoints","content":"Securing Data Connect Endpoints A future version of Data Connect will document how to use GA4GH Passports and Visas to authenticate and authorize requests to the API.\nThere is already work underway to specify how DRS will work with Passports. Rather than jumping in now and creating confusion, the Data Connect working group is monitoring the Passport efforts in DRS and will document a specific Data Connect Passport integration that makes sense in the context of what has been decided for DRS.\nFor now, prefer JSON Web Tokens (JWTs) presented as OAuth2 bearer tokens on each Data Connect API request. This will likely put you in a good position to implement the recommended Passport integration when the path forward becomes clear.\n"},{"url":"/docs/getting-started/introduction/","title":"Introduction","content":"{row-divider}\nData Connect is a standard for discovery and search of biomedical data, developed by the Discovery Work Stream of the Global Alliance for Genomics \u0026amp; Health.\nThe standard provides a mechanism for:\n Describing data and its data model.  Data Connect\u0026rsquo;s Table API component provides a way to organize data into \u0026ldquo;Tables\u0026rdquo; and describe their data model, leveraging the JSON Schema standard.   Searching the data with the given data model.  Data Connect\u0026rsquo;s Search API component provides a way to query \u0026ldquo;Tables\u0026rdquo; of data, leveraging the SQL standard.    It is not in the scope of the standard to:\n Define said data models.  Data Connect relies on other efforts in GA4GH (e.g. GA4GH SchemaBlocks), as well as outside implementers.    Background GA4GH has previously developed two standards for discovery. Beacon is a standard for discovery of genomic variants, while Matchmaker is a standard for discovery of subjects with certain genomic and phenotypic features. Implementations of these standards have been linked into federated networks (e.g. Beacon Network and Matchmaker Exchange).\nBoth standards (and the corresponding networks) have been successful in their own right, but had a lot in common. It was acknowledged that it would be broadly useful to develop standards that abstract common infrastructure for building searchable, federated networks for a variety of applications in genomics and health.\nData Connect, formerly known as GA4GH Search, is this general-purpose middleware for building federated, search-based applications. The name of the API reflects its purpose of:\n Giving data providers a mechanism to enable others to connect to their data via the described data models. Allowing data consumers to make connections within the data through a flexible query language.  Benefits  Interoperable. Simple, interoperable, uniform mechanism to publish, discover, and search biomedical data. Flexible. Works with any data that can be serialized as an array of JSON objects. Recommends the use of GA4GH SchemaBlocks data models, but allows custodians to specify their own data models to make their data available without extensive ETL transformations. Supports federation. Serves as a general-purpose framework for building federatable search-based applications across multiple implementations. Federations reference common schemas and properties. Minimal by design. The API is purposely kept minimal so that the barriers to publishing existing data are as small as possible. Backend agnostic. It is possible to implement the API across a large variety of backend datastores. General purpose. Admits use cases that have not yet been thought of.  Use cases Data Connect is an intentionally general-purpose middleware meant to enable the development of a diverse ecosystem of applications.\nThe community has built versions of the following applications on top of Data Connect:\n Data Explorers Beacons Patient matchmaking Jupyter notebooks R data frames Command line query tools Data and metadata indexers Data federations Concept cross-references  We\u0026rsquo;re looking forward to seeing things we haven’t yet imagined!\nThe community has also connected data through the following data sources:\n FHIR Relational databases CSV/TSV files with data dictionaries VCF+TBI files Phenopackets Google BigQuery Google Sheets and more!  Examples of queries on the data that can be answered via Data Connect include:\n Find subjects with HP:0001519 and candidate gene FBN1 (use case of Matchmaker Exchange) Find male subjects with HP:0009726 consented for General Research Use (use case of European Genome-phenome Archive) Find adult males diagnosed with autism having a harmful mutation in SHANK1 (use case of Autism Sharing Initiative) Find dataset from subject on European data center hosted on Amazon (use case of Cloud Work Stream)  Full summary of use cases can be found in USECASES.md.\n{divider} Quick Links  Specification\nInstalling Client Libraries\nPublishing Data Examples\nData Consumption Examples\n \n"},{"url":"/docs/reference/pagination-long-queries/","title":"Pagination and Long Running Queries","content":"Pagination Sequence\nA pagination sequence is the singly-linked list of URLs formed by following the next_page_url property of the pagination section of an initial TableData or ListTablesResponse. A pagination sequence begins at the first response returned from any request that yields a TableData or ListTablesResponse, and ends at the page in the sequence whose pagination property is omitted, whose pagination.next_page_url is omitted, or whose pagination.next_page_url is null.\nServers may return a unique pagination sequence in response to successive requests for the same query, table data listing, or table listing.\nExcept for the last page, pagination.next_page_url property must be either an absolute URL or a relative reference as defined by RFC 3986 section 4.2 whose base URL is the URL that the page containing the reference was fetched from.\nEvery non-empty TableData page in a pagination sequence must include a data_model property. If present, the data_model property must be a valid JSON Schema.\nAcross all TableData pages in the pagination sequence that have a data_model value, the data_models must be identical. Some TableData pages may lack a data_model. See the empty page rules below.\nServers may respond with an HTTP 4xx error code if the same page is requested more than once.\nDue to both rules above, clients must not rely on the ability to re-fetch previously encountered pages.\nServers may include a Retry-After HTTP header in each response that is part of a pagination sequence, and clients must respect the delay specified by such header before attempting to fetch the next page.\nEmpty TableData Pages\nWhile many types of queries will be completed quickly, others will take minutes or even hours to yield a result. The simplest solution would be a synchronous design: query requests block until data is ready, then return a TableData response with the initial rows of the result set. However, asking clients to block for hours on a single HTTP response is fraught with difficulty: open connections are costly and fragile. If an intermediary times out the request, the results will be lost and the client must start over.\nTo allow servers to direct clients to poll for results rather than hold open HTTP connections for long-running queries, the following special pagination rules apply to empty pages.\nAn empty page is defined as a TableData object whose data property is a zero element array.\nA pagination sequence MAY include any number of empty pages anywhere in the sequence.\nAn empty TableData page may omit its data_model property entirely. This allows servers to direct clients to poll for results before the result schema has been determined.\nA server that returns an empty page should include a Retry-After header in the HTTP response. If a client encounters an empty page with no Retry-After header, the client should delay at least 1 second before requesting the next page.\nExample: Server returning empty pages to make client poll\nThis example illustrates a server returning a series of empty pages to a client while it is preparing the result set. The client polls for results by following next_page_url at the rate specified by the server. The form of the pagination URLs are only an example of one possible scheme. Servers are free to employ any pagination URL scheme.\nInitial Request\nPOST /search content-type: application/json {\u0026#34;query\u0026#34;:\u0026#34;select distinct gene_symbol from example_project.brca_exchange.v32\u0026#34;} HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/queued/1\u0026#34;}} 2nd request (Polling after sleeping for 1000ms)\nGET /search/v1/statement/abc123/queued/1 HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/queued/2\u0026#34;}} 3rd request (Polling again after sleeping for 1000ms)\nGET /search/v1/statement/abc123/queued/2 HTTP/1.1 200 OK content-type: application/json retry-after: 1000 {\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/executing/1\u0026#34;}} 4th request (Polling again after sleeping for 1000ms)\nGET /search/v1/statement/abc123/executing/1 HTTP/1.1 200 OK content-type: application/json {\u0026#34;data_model\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Automatically generated schema\u0026#34;,\u0026#34;$schema\u0026#34;:\u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;gene_symbol\u0026#34;:{\u0026#34;format\u0026#34;:\u0026#34;varchar\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}}},\u0026#34;data\u0026#34;:[{\u0026#34;gene_symbol\u0026#34;:\u0026#34;BRCA2\u0026#34;},{\u0026#34;gene_symbol\u0026#34;:\u0026#34;BRCA1\u0026#34;}],\u0026#34;pagination\u0026#34;:{\u0026#34;next_page_url\u0026#34;:\u0026#34;/search/v1/statement/abc123/executing/2\u0026#34;}} Final request (no delay because page was nonempty and no retry-after header was present on the response)\nGET /search/v1/statement/abc123/executing/2 HTTP/1.1 200 OK content-type: application/json {\u0026#34;data_model\u0026#34;:{\u0026#34;description\u0026#34;:\u0026#34;Automatically generated schema\u0026#34;,\u0026#34;$schema\u0026#34;:\u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;,\u0026#34;properties\u0026#34;:{\u0026#34;gene_symbol\u0026#34;:{\u0026#34;format\u0026#34;:\u0026#34;varchar\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;}}},\u0026#34;data\u0026#34;:[],\u0026#34;pagination\u0026#34;:{}} Example: Client algorithm for consuming TableData pages\nThe algorithm provided here simply illustrates one way to comply with the rules above. Any algorithm that satisfies all rules acceptable.\n Start with an empty data buffer and undefined data model. Loop:  If the response is an error, report the error and abort If no data_model has been seen so far, check if this page contains a data_model. If so, define the data model for the whole pagination sequence as this page’s data_model. Append the row data from the current page to the data buffer (there may be 0 rows on any given page) Delay for the time specified in the Retry-After HTTP response header for the current page (default is no delay) If there is a pagination object and it has a non-null next_page_url, fetch that URL, make that response the current page, and start back at step 2a; otherwise end.    "},{"url":"/docs/reference/sql-functions/","title":"SQL Functions","content":"Data Connect\u0026rsquo;s SQL dialect has been selected for compatibility with current major open source database platforms including Trino, PostgreSQL, and MySQL, and BigQuery. There are occasional name or signature differences, but a Data Connect implementation atop any of the major database platforms should be able to pass through queries that use the functions listed below with only minor tweaks.\nThe functions below are a subset of those available in Trino 341. In a conformant Data Connect implementation, these functions must behave according to the Trino documentation. To assist with implementations directly on other database platforms, the Trino Functions Support Matrix captures the differences between platforms in granular detail.\n Logical Operators  AND, OR, NOT   Comparison Operators  \u0026amp;lt;, \u0026gt;, \u0026amp;lt;=, \u0026gt;=, =, \u0026amp;lt;\u0026gt;, != BETWEEN, IS NULL, IS NOT NULL IS DISTINCT FROM* IS NOT DISTINCT FROM* GREATEST, LEAST Quantified Comparison Predicates: ALL, ANY and SOME* Pattern Comparison: LIKE   Conditional Expressions  CASE, IF, COALESCE, NULLIF   Conversion Functions  cast(value AS type) → type format(format, args...) → varchar   Mathematical Functions  Most basic functions are supported across implementations. Notably missing are hyperbolic trig functions, infinity, floating point, and statistical/CDF functions. abs(x) → [same as input] ceil(x) → [same as input] ceiling(x) → [same as input] degrees(x) → double* exp(x) → double floor(x) → [same as input] ln(x) → double log(b, x) → double log10(x) → double mod(n, m) → [same as input] pi() → double pow(x, p) → double* power(x, p) → double radians(x) → double* round(x) → [same as input] round(x, d) → [same as input] sign(x) → [same as input] sqrt(x) → double truncate(x) → double* Random Functions:  rand() → double* random() → double* random(n) → [same as input]* random(m, n) → [same as input]*   Trigonometric Functions:  acos(x) → double asin(x) → double atan(x) → double atan2(y, x) → double cos(x) → double sin(x) → double tan(x) → double     Bitwise Functions  bitwise_and(x, y) → bigint bitwise_or(x, y) → bigint bitwise_xor(x, y) → bigint bitwise_not(x) → bigint bitwise_left_shift(value, shift) → [same as value] bitwise_right_shift(value, shift, digits) → [same as value] bit_count(x, bits) → bigint*   Regular Expression Functions  regexp_extract_all(string, pattern) -\u0026gt; array(varchar)* regexp_extract_all(string, pattern, group) -\u0026gt; array(varchar)* regexp_extract(string, pattern) → varchar* regexp_extract(string, pattern, group) → varchar* regexp_like(string, pattern) → boolean* regexp_replace(string, pattern) → varchar* regexp_replace(string, pattern, replacement) → varchar* regexp_replace(string, pattern, function) → varchar*   UUID Functions  uuid()*   Session Information Functions  current_user*   String manipulation  Operators:  Concatenation (||)* LIKE   Functions:  chr(n) → varchar* codepoint(string) → integer* format(format, args...) → varchar length(string) → bigint lower(string) → varchar lpad(string, size, padstring) → varchar ltrim(string) → varchar position(substring IN string) → bigint* replace(string, search, replace) → varchar reverse(string) → varchar rpad(string, size, padstring) → varchar rtrim(string) → varchar split(string, delimiter, limit) -\u0026gt; array(varchar)* starts_with(string, substring) → boolean* strpos(string, substring) → bigint* substr(string, start) → varchar* substring(string, start) → varchar substr(string, start, length) → varchar* substring(string, start, length) → varchar trim(string) → varchar upper(string) → varchar     Date manipulation   Be aware of different quotation (') syntax requirements between MySQL and PostgreSQL. BigQuery does not support the +/- operators for dates. Convenience methods could be replaced with EXTRACT().\n  Operators:  +, - * AT TIME ZONE*   Functions:  current_date current_time current_timestamp current_timestamp(p)* date(x) → date* date_trunc(unit, x) → [same as input]* date_add(unit, value, timestamp) → [same as input]* date_diff(unit, timestamp1, timestamp2) → bigint* extract(field FROM x) → bigint* from_unixtime(unixtime) -\u0026gt; timestamp(3)* from_unixtime(unixtime, zone) → timestamp(3) with time zone* from_unixtime(unixtime, hours, minutes) → timestamp(3) with time zone* Localtime* localtimestamp* localtimestamp(p)* now() → timestamp(3) with time zone* to_unixtime(timestamp) → double*   MySQL-like date functions:  date_format(timestamp, format) → varchar* date_parse(string, format) → timestamp(3)*   Aggregate functions **Note that Trino provides a much larger superset of functions. Bitwise, map, and approximate aggregations are mostly absent. Only BigQuery has a few native approximate aggregation functions.  array_agg(x) → array\u0026amp;lt;[same as input]\u0026gt;* avg(x) → double bool_and(boolean) → boolean* bool_or(boolean) → boolean* count(*) → bigint* count(x) → bigint count_if(x) → bigint* every(boolean) → boolean* max(x) → [same as input] max(x, n) → array\u0026amp;lt;[same as x]\u0026gt;* min(x) → [same as input] min(x, n) → array\u0026amp;lt;[same as x]\u0026gt;* sum(x) → [same as input] Statistical Aggregate Functions:  corr(y, x) → double* covar_pop(y, x)→ double* covar_samp(y, x) → double* stddev(x) → double stddev_pop(x) → double stddev_samp(x) → double variance(x) → double var_pop(x) → double var_samp(x) → double     Window functions  Ranking Functions:  cume_dist() → bigint dense_rank() → bigint ntile(n) → bigint percent_rank() → double rank() → bigint row_number() → bigint   Value Functions:  first_value(x) → [same as input] last_value(x) → [same as input] nth_value(x, offset) → [same as input] lead(x[, offset[, default_value]]) → [same as input] lag(x[, offset[, default_value]]) → [same as input]     JSON functions In general, function signatures and behaviour differs across implementations for many JSON related functions.  json_array_length(json) → bigint* json_extract(json, json_path) → json* json_extract_scalar(json, json_path) → varchar* json_format(json) → varchar* json_size(json, json_path) → bigint*   Functions for working with nested and repeated data (ROW and ARRAY) See also UNNEST, which is part of the SQL grammar and allows working with nested arrays as if they were rows in a joined table.  Note: Arrays are mostly absent in MySQL * Array Subscript Operator: [] * Array Concatenation Operator: || * concat(array1, array2, ..., arrayN) → array * cardinality(x) → bigint*\n ga4gh_type described in the Data Connect specification  "},{"url":"/docs/reference/sql-grammar/","title":"SQL Grammar","content":"This is the ANTLR grammar from Trino version 323 (ASL 2.0 license), with the DML and DDL parts removed.\ngrammar DataConnect; tokens { DELIMITER } singleStatement : statement EOF ; standaloneExpression : expression EOF ; standaloneType : type EOF ; statement : query #statementDefault | USE schema=identifier #use | USE catalog=identifier '.' schema=identifier #use | EXPLAIN ANALYZE? VERBOSE? ('(' explainOption (',' explainOption)* ')')? statement #explain | SHOW TABLES ((FROM | IN) qualifiedName)? (LIKE pattern=string (ESCAPE escape=string)?)? #showTables | SHOW SCHEMAS ((FROM | IN) identifier)? (LIKE pattern=string (ESCAPE escape=string)?)? #showSchemas | SHOW CATALOGS (LIKE pattern=string)? #showCatalogs | SHOW COLUMNS (FROM | IN) qualifiedName #showColumns | DESCRIBE qualifiedName #showColumns | DESC qualifiedName #showColumns | SHOW FUNCTIONS #showFunctions ; query : with? queryNoWith ; with : WITH RECURSIVE? namedQuery (',' namedQuery)* ; queryNoWith: queryTerm (ORDER BY sortItem (',' sortItem)*)? (OFFSET offset=INTEGER_VALUE (ROW | ROWS)?)? ((LIMIT limit=(INTEGER_VALUE | ALL)) | (FETCH (FIRST | NEXT) (fetchFirst=INTEGER_VALUE)? (ROW | ROWS) (ONLY | WITH TIES)))? ; queryTerm : queryPrimary #queryTermDefault | left=queryTerm operator=INTERSECT setQuantifier? right=queryTerm #setOperation | left=queryTerm operator=(UNION | EXCEPT) setQuantifier? right=queryTerm #setOperation ; queryPrimary : querySpecification #queryPrimaryDefault | TABLE qualifiedName #table | VALUES expression (',' expression)* #inlineTable | '(' queryNoWith ')' #subquery ; sortItem : expression ordering=(ASC | DESC)? (NULLS nullOrdering=(FIRST | LAST))? ; querySpecification : SELECT setQuantifier? selectItem (',' selectItem)* (FROM relation (',' relation)*)? (WHERE where=booleanExpression)? (GROUP BY groupBy)? (HAVING having=booleanExpression)? ; groupBy : setQuantifier? groupingElement (',' groupingElement)* ; groupingElement : groupingSet #singleGroupingSet | ROLLUP '(' (expression (',' expression)*)? ')' #rollup | CUBE '(' (expression (',' expression)*)? ')' #cube | GROUPING SETS '(' groupingSet (',' groupingSet)* ')' #multipleGroupingSets ; groupingSet : '(' (expression (',' expression)*)? ')' | expression ; namedQuery : name=identifier (columnAliases)? AS '(' query ')' ; setQuantifier : DISTINCT | ALL ; selectItem : expression (AS? identifier)? #selectSingle | primaryExpression '.' ASTERISK (AS columnAliases)? #selectAll | ASTERISK #selectAll ; relation : left=relation ( CROSS JOIN right=sampledRelation | joinType JOIN rightRelation=relation joinCriteria | NATURAL joinType JOIN right=sampledRelation ) #joinRelation | sampledRelation #relationDefault ; joinType : INNER? | LEFT OUTER? | RIGHT OUTER? | FULL OUTER? ; joinCriteria : ON booleanExpression | USING '(' identifier (',' identifier)* ')' ; sampledRelation : aliasedRelation ( TABLESAMPLE sampleType '(' percentage=expression ')' )? ; sampleType : BERNOULLI | SYSTEM ; aliasedRelation : relationPrimary (AS? identifier columnAliases?)? ; columnAliases : '(' identifier (',' identifier)* ')' ; relationPrimary : qualifiedName #tableName | '(' query ')' #subqueryRelation | UNNEST '(' expression (',' expression)* ')' (WITH ORDINALITY)? #unnest | LATERAL '(' query ')' #lateral | '(' relation ')' #parenthesizedRelation ; expression : booleanExpression ; booleanExpression : valueExpression predicate[$valueExpression.ctx]? #predicated | NOT booleanExpression #logicalNot | left=booleanExpression operator=AND right=booleanExpression #logicalBinary | left=booleanExpression operator=OR right=booleanExpression #logicalBinary ; // workaround for https://github.com/antlr/antlr4/issues/780 predicate[ParserRuleContext value] : comparisonOperator right=valueExpression #comparison | comparisonOperator comparisonQuantifier '(' query ')' #quantifiedComparison | NOT? BETWEEN lower=valueExpression AND upper=valueExpression #between | NOT? IN '(' expression (',' expression)* ')' #inList | NOT? IN '(' query ')' #inSubquery | NOT? LIKE pattern=valueExpression (ESCAPE escape=valueExpression)? #like | IS NOT? NULL #nullPredicate | IS NOT? DISTINCT FROM right=valueExpression #distinctFrom ; valueExpression : primaryExpression #valueExpressionDefault | valueExpression AT timeZoneSpecifier #atTimeZone | operator=(MINUS | PLUS) valueExpression #arithmeticUnary | left=valueExpression operator=(ASTERISK | SLASH | PERCENT) right=valueExpression #arithmeticBinary | left=valueExpression operator=(PLUS | MINUS) right=valueExpression #arithmeticBinary | left=valueExpression CONCAT right=valueExpression #concatenation ; primaryExpression : NULL #nullLiteral | interval #intervalLiteral | identifier string #typeConstructor | DOUBLE PRECISION string #typeConstructor | number #numericLiteral | booleanValue #booleanLiteral | string #stringLiteral | BINARY_LITERAL #binaryLiteral | '?' #parameter | POSITION '(' valueExpression IN valueExpression ')' #position | '(' expression (',' expression)+ ')' #rowConstructor | ROW '(' expression (',' expression)* ')' #rowConstructor | qualifiedName '(' ASTERISK ')' filter? over? #functionCall | qualifiedName '(' (setQuantifier? expression (',' expression)*)? (ORDER BY sortItem (',' sortItem)*)? ')' filter? (nullTreatment? over)? #functionCall | identifier '-\u0026gt;' expression #lambda | '(' (identifier (',' identifier)*)? ')' '-\u0026gt;' expression #lambda | '(' query ')' #subqueryExpression // This is an extension to ANSI SQL, which considers EXISTS to be a \u0026lt;boolean expression\u0026gt; | EXISTS '(' query ')' #exists | CASE operand=expression whenClause+ (ELSE elseExpression=expression)? END #simpleCase | CASE whenClause+ (ELSE elseExpression=expression)? END #searchedCase | CAST '(' expression AS type ')' #cast | TRY_CAST '(' expression AS type ')' #cast | ARRAY '[' (expression (',' expression)*)? ']' #arrayConstructor | value=primaryExpression '[' index=valueExpression ']' #subscript | identifier #columnReference | base=primaryExpression '.' fieldName=identifier #dereference | name=CURRENT_DATE #specialDateTimeFunction | name=CURRENT_TIME ('(' precision=INTEGER_VALUE ')')? #specialDateTimeFunction | name=CURRENT_TIMESTAMP ('(' precision=INTEGER_VALUE ')')? #specialDateTimeFunction | name=LOCALTIME ('(' precision=INTEGER_VALUE ')')? #specialDateTimeFunction | name=LOCALTIMESTAMP ('(' precision=INTEGER_VALUE ')')? #specialDateTimeFunction | name=CURRENT_USER #currentUser | name=CURRENT_PATH #currentPath | SUBSTRING '(' valueExpression FROM valueExpression (FOR valueExpression)? ')' #substring | NORMALIZE '(' valueExpression (',' normalForm)? ')' #normalize | EXTRACT '(' identifier FROM valueExpression ')' #extract | '(' expression ')' #parenthesizedExpression | GROUPING '(' (qualifiedName (',' qualifiedName)*)? ')' #groupingOperation ; nullTreatment : IGNORE NULLS | RESPECT NULLS ; string : STRING #basicStringLiteral | UNICODE_STRING (UESCAPE STRING)? #unicodeStringLiteral ; timeZoneSpecifier : TIME ZONE interval #timeZoneInterval | TIME ZONE string #timeZoneString ; comparisonOperator : EQ | NEQ | LT | LTE | GT | GTE ; comparisonQuantifier : ALL | SOME | ANY ; booleanValue : TRUE | FALSE ; interval : INTERVAL sign=(PLUS | MINUS)? string from=intervalField (TO to=intervalField)? ; intervalField : YEAR | MONTH | DAY | HOUR | MINUTE | SECOND ; normalForm : NFD | NFC | NFKD | NFKC ; type : ROW '(' rowField (',' rowField)* ')' #rowType | INTERVAL from=intervalField (TO to=intervalField)? #intervalType | base=TIMESTAMP ('(' precision = INTEGER_VALUE ')')? (WITHOUT TIME ZONE)? #dateTimeType | base=TIMESTAMP ('(' precision = INTEGER_VALUE ')')? WITH TIME ZONE #dateTimeType | base=TIME ('(' precision = INTEGER_VALUE ')')? (WITHOUT TIME ZONE)? #dateTimeType | base=TIME ('(' precision = INTEGER_VALUE ')')? WITH TIME ZONE #dateTimeType | DOUBLE PRECISION #doublePrecisionType | ARRAY '\u0026lt;' type '\u0026gt;' #legacyArrayType | MAP '\u0026lt;' keyType=type ',' valueType=type '\u0026gt;' #legacyMapType | type ARRAY ('[' INTEGER_VALUE ']')? #arrayType | identifier ('(' typeParameter (',' typeParameter)* ')')? #genericType ; rowField : identifier? type; typeParameter : INTEGER_VALUE | type ; whenClause : WHEN condition=expression THEN result=expression ; filter : FILTER '(' WHERE booleanExpression ')' ; over : OVER '(' (PARTITION BY partition+=expression (',' partition+=expression)*)? (ORDER BY sortItem (',' sortItem)*)? windowFrame? ')' ; windowFrame : frameType=RANGE start=frameBound | frameType=ROWS start=frameBound | frameType=RANGE BETWEEN start=frameBound AND end=frameBound | frameType=ROWS BETWEEN start=frameBound AND end=frameBound ; frameBound : UNBOUNDED boundType=PRECEDING #unboundedFrame | UNBOUNDED boundType=FOLLOWING #unboundedFrame | CURRENT ROW #currentRowBound | expression boundType=(PRECEDING | FOLLOWING) #boundedFrame ; explainOption : FORMAT value=(TEXT | GRAPHVIZ | JSON) #explainFormat | TYPE value=(LOGICAL | DISTRIBUTED | VALIDATE | IO) #explainType ; qualifiedName : identifier ('.' identifier)* ; identifier : IDENTIFIER #unquotedIdentifier | QUOTED_IDENTIFIER #quotedIdentifier | nonReserved #unquotedIdentifier | BACKQUOTED_IDENTIFIER #backQuotedIdentifier | DIGIT_IDENTIFIER #digitIdentifier ; number : MINUS? DECIMAL_VALUE #decimalLiteral | MINUS? DOUBLE_VALUE #doubleLiteral | MINUS? INTEGER_VALUE #integerLiteral ; nonReserved // IMPORTANT: this rule must only contain tokens. Nested rules are not supported. See SqlParser.exitNonReserved : ADD | ADMIN | ALL | ANALYZE | ANY | ARRAY | ASC | AT | BERNOULLI | CALL | CASCADE | CATALOGS | COLUMN | COLUMNS | COMMENT | COMMIT | COMMITTED | CURRENT | DATA | DATE | DAY | DEFINER | DESC | DISTRIBUTED | DOUBLE | EXCLUDING | EXPLAIN | FETCH | FILTER | FIRST | FOLLOWING | FORMAT | FUNCTIONS | GRANT | GRANTED | GRANTS | GRAPHVIZ | HOUR | IF | IGNORE | INCLUDING | INPUT | INTERVAL | INVOKER | IO | ISOLATION | JSON | LAST | LATERAL | LEVEL | LIMIT | LOGICAL | MAP | MINUTE | MONTH | NEXT | NFC | NFD | NFKC | NFKD | NO | NONE | NULLIF | NULLS | OFFSET | ONLY | OPTION | ORDINALITY | OUTPUT | OVER | PARTITION | PARTITIONS | PATH | POSITION | PRECEDING | PRECISION | PRIVILEGES | PROPERTIES | RANGE | READ | RENAME | REPEATABLE | REPLACE | RESET | RESPECT | RESTRICT | REVOKE | ROLE | ROLES | ROLLBACK | ROW | ROWS | SCHEMA | SCHEMAS | SECOND | SECURITY | SERIALIZABLE | SESSION | SET | SETS | SHOW | SOME | START | STATS | SUBSTRING | SYSTEM | TABLES | TABLESAMPLE | TEXT | TIES | TIME | TIMESTAMP | TO | TRANSACTION | TRY_CAST | TYPE | UNBOUNDED | UNCOMMITTED | USE | USER | VALIDATE | VERBOSE | VIEW | WITHOUT | WORK | WRITE | YEAR | ZONE ; ADD: 'ADD'; ADMIN: 'ADMIN'; ALL: 'ALL'; ALTER: 'ALTER'; ANALYZE: 'ANALYZE'; AND: 'AND'; ANY: 'ANY'; ARRAY: 'ARRAY'; AS: 'AS'; ASC: 'ASC'; AT: 'AT'; BERNOULLI: 'BERNOULLI'; BETWEEN: 'BETWEEN'; BY: 'BY'; CALL: 'CALL'; CASCADE: 'CASCADE'; CASE: 'CASE'; CAST: 'CAST'; CATALOGS: 'CATALOGS'; COLUMN: 'COLUMN'; COLUMNS: 'COLUMNS'; COMMENT: 'COMMENT'; COMMIT: 'COMMIT'; COMMITTED: 'COMMITTED'; CONSTRAINT: 'CONSTRAINT'; CREATE: 'CREATE'; CROSS: 'CROSS'; CUBE: 'CUBE'; CURRENT: 'CURRENT'; CURRENT_DATE: 'CURRENT_DATE'; CURRENT_PATH: 'CURRENT_PATH'; CURRENT_ROLE: 'CURRENT_ROLE'; CURRENT_TIME: 'CURRENT_TIME'; CURRENT_TIMESTAMP: 'CURRENT_TIMESTAMP'; CURRENT_USER: 'CURRENT_USER'; DATA: 'DATA'; DATE: 'DATE'; DAY: 'DAY'; DEALLOCATE: 'DEALLOCATE'; DEFINER: 'DEFINER'; DELETE: 'DELETE'; DESC: 'DESC'; DESCRIBE: 'DESCRIBE'; DISTINCT: 'DISTINCT'; DISTRIBUTED: 'DISTRIBUTED'; DOUBLE: 'DOUBLE'; DROP: 'DROP'; ELSE: 'ELSE'; END: 'END'; ESCAPE: 'ESCAPE'; EXCEPT: 'EXCEPT'; EXCLUDING: 'EXCLUDING'; EXECUTE: 'EXECUTE'; EXISTS: 'EXISTS'; EXPLAIN: 'EXPLAIN'; EXTRACT: 'EXTRACT'; FALSE: 'FALSE'; FETCH: 'FETCH'; FILTER: 'FILTER'; FIRST: 'FIRST'; FOLLOWING: 'FOLLOWING'; FOR: 'FOR'; FORMAT: 'FORMAT'; FROM: 'FROM'; FULL: 'FULL'; FUNCTIONS: 'FUNCTIONS'; GRANT: 'GRANT'; GRANTED: 'GRANTED'; GRANTS: 'GRANTS'; GRAPHVIZ: 'GRAPHVIZ'; GROUP: 'GROUP'; GROUPING: 'GROUPING'; HAVING: 'HAVING'; HOUR: 'HOUR'; IF: 'IF'; IGNORE: 'IGNORE'; IN: 'IN'; INCLUDING: 'INCLUDING'; INNER: 'INNER'; INPUT: 'INPUT'; INSERT: 'INSERT'; INTERSECT: 'INTERSECT'; INTERVAL: 'INTERVAL'; INTO: 'INTO'; INVOKER: 'INVOKER'; IO: 'IO'; IS: 'IS'; ISOLATION: 'ISOLATION'; JSON: 'JSON'; JOIN: 'JOIN'; LAST: 'LAST'; LATERAL: 'LATERAL'; LEFT: 'LEFT'; LEVEL: 'LEVEL'; LIKE: 'LIKE'; LIMIT: 'LIMIT'; LOCALTIME: 'LOCALTIME'; LOCALTIMESTAMP: 'LOCALTIMESTAMP'; LOGICAL: 'LOGICAL'; MAP: 'MAP'; MINUTE: 'MINUTE'; MONTH: 'MONTH'; NATURAL: 'NATURAL'; NEXT: 'NEXT'; NFC : 'NFC'; NFD : 'NFD'; NFKC : 'NFKC'; NFKD : 'NFKD'; NO: 'NO'; NONE: 'NONE'; NORMALIZE: 'NORMALIZE'; NOT: 'NOT'; NULL: 'NULL'; NULLIF: 'NULLIF'; NULLS: 'NULLS'; OFFSET: 'OFFSET'; ON: 'ON'; ONLY: 'ONLY'; OPTION: 'OPTION'; OR: 'OR'; ORDER: 'ORDER'; ORDINALITY: 'ORDINALITY'; OUTER: 'OUTER'; OUTPUT: 'OUTPUT'; OVER: 'OVER'; PARTITION: 'PARTITION'; PARTITIONS: 'PARTITIONS'; PATH: 'PATH'; POSITION: 'POSITION'; PRECEDING: 'PRECEDING'; PREPARE: 'PREPARE'; PRIVILEGES: 'PRIVILEGES'; PRECISION: 'PRECISION'; PROPERTIES: 'PROPERTIES'; RANGE: 'RANGE'; READ: 'READ'; RECURSIVE: 'RECURSIVE'; RENAME: 'RENAME'; REPEATABLE: 'REPEATABLE'; REPLACE: 'REPLACE'; RESET: 'RESET'; RESPECT: 'RESPECT'; RESTRICT: 'RESTRICT'; REVOKE: 'REVOKE'; RIGHT: 'RIGHT'; ROLE: 'ROLE'; ROLES: 'ROLES'; ROLLBACK: 'ROLLBACK'; ROLLUP: 'ROLLUP'; ROW: 'ROW'; ROWS: 'ROWS'; SCHEMA: 'SCHEMA'; SCHEMAS: 'SCHEMAS'; SECOND: 'SECOND'; SECURITY: 'SECURITY'; SELECT: 'SELECT'; SERIALIZABLE: 'SERIALIZABLE'; SESSION: 'SESSION'; SET: 'SET'; SETS: 'SETS'; SHOW: 'SHOW'; SOME: 'SOME'; START: 'START'; STATS: 'STATS'; SUBSTRING: 'SUBSTRING'; SYSTEM: 'SYSTEM'; TABLE: 'TABLE'; TABLES: 'TABLES'; TABLESAMPLE: 'TABLESAMPLE'; TEXT: 'TEXT'; THEN: 'THEN'; TIES: 'TIES'; TIME: 'TIME'; TIMESTAMP: 'TIMESTAMP'; TO: 'TO'; TRANSACTION: 'TRANSACTION'; TRUE: 'TRUE'; TRY_CAST: 'TRY_CAST'; TYPE: 'TYPE'; UESCAPE: 'UESCAPE'; UNBOUNDED: 'UNBOUNDED'; UNCOMMITTED: 'UNCOMMITTED'; UNION: 'UNION'; UNNEST: 'UNNEST'; USE: 'USE'; USER: 'USER'; USING: 'USING'; VALIDATE: 'VALIDATE'; VALUES: 'VALUES'; VERBOSE: 'VERBOSE'; VIEW: 'VIEW'; WHEN: 'WHEN'; WHERE: 'WHERE'; WITH: 'WITH'; WITHOUT: 'WITHOUT'; WORK: 'WORK'; WRITE: 'WRITE'; YEAR: 'YEAR'; ZONE: 'ZONE'; EQ : '='; NEQ : '\u0026lt;\u0026gt;' | '!='; LT : '\u0026lt;'; LTE : '\u0026lt;='; GT : '\u0026gt;'; GTE : '\u0026gt;='; PLUS: '+'; MINUS: '-'; ASTERISK: '*'; SLASH: '/'; PERCENT: '%'; CONCAT: '||'; STRING : '\\'' ( ~'\\'' | '\\'\\'' )* '\\'' ; UNICODE_STRING : 'U\u0026amp;\\'' ( ~'\\'' | '\\'\\'' )* '\\'' ; // Note: we allow any character inside the binary literal and validate // its a correct literal when the AST is being constructed. This // allows us to provide more meaningful error messages to the user BINARY_LITERAL : 'X\\'' (~'\\'')* '\\'' ; INTEGER_VALUE : DIGIT+ ; DECIMAL_VALUE : DIGIT+ '.' DIGIT* | '.' DIGIT+ ; DOUBLE_VALUE : DIGIT+ ('.' DIGIT*)? EXPONENT | '.' DIGIT+ EXPONENT ; IDENTIFIER : (LETTER | '_') (LETTER | DIGIT | '_' | '@' | ':')* ; DIGIT_IDENTIFIER : DIGIT (LETTER | DIGIT | '_' | '@' | ':')+ ; QUOTED_IDENTIFIER : '\u0026quot;' ( ~'\u0026quot;' | '\u0026quot;\u0026quot;' )* '\u0026quot;' ; BACKQUOTED_IDENTIFIER : '`' ( ~'`' | '``' )* '`' ; fragment EXPONENT : 'E' [+-]? DIGIT+ ; fragment DIGIT : [0-9] ; fragment LETTER : [A-Z] ; SIMPLE_COMMENT : '--' ~[\\r\\n]* '\\r'? '\\n'? -\u0026gt; channel(HIDDEN) ; BRACKETED_COMMENT : '/*' .*? '*/' -\u0026gt; channel(HIDDEN) ; WS : [ \\r\\n\\t]+ -\u0026gt; channel(HIDDEN) ; // Catch-all for anything we can't recognize. // We use this to be able to ignore and recover all the text // when splitting statements with DelimiterLexer UNRECOGNIZED : . ; "},{"url":"/docs/use-existing-data/using-trino/doc/","title":"Using Trino","content":"The dbGaP GECCO Example In the provision data section, we\u0026rsquo;ve shown a quick start recipe with the data-connect-trino docker container connected to a Trino instance hosted at https://trino-public.prod.dnastack.com. This section provides more information on how this was accomplished.\nQuick Links  data-connect-trino\nOpen API 3 Reference\nFull Data Connect Specification\nTable Object Specification\nData Connect API’s SQL dialect\n  Prerequisites The following is required before we start.\n Java 11+ A Trino server you can access anonymously over HTTP(S). Git   If you don\u0026rsquo;t have a Trino server to work against and you wish to try the app, try using https://trino-public.prod.dnastack.com as the data source.\n 1. Building the Trino Adapter App\nClone the repository\ngit clone https://github.com/DNAstack/data-connect-trino.git Build the app\n./mvnw clean package 2. Configuration\nFor a minimal configuration, we need a local PostgreSQL database where the app stores its bookkeeping information. By default, the app looks for a local PostgreSQL instance at localhost:5432 with username, password, and database name dataconnecttrino. You can spin up such a database with this docker command:\ndocker run -d -p 5432:5432 --name data-connect-app-db -e POSTGRES_USER=dataconnecttrino -e POSTGRES_PASSWORD=dataconnecttrino postgres Now you only need to provide two parameters: PRESTO_DATASOURCE_URL and SPRING_PROFILES_ACTIVE.\nPRESTO_DATASOURCE_URL points to the Trino server you wish to expose with a Data Connect API.\nSPRING_PROFILES_ACTIVE is used to disable OAuth authentication, which simplifies first-time setup.\nexport PRESTO_DATASOURCE_URL=https://\u0026lt;your-presto-server\u0026gt; export SPRING_PROFILES_ACTIVE=no-auth 3. Run the adapter App\n./mvnw clean spring-boot:run Your application should now be accessible at http://localhost:8089/tables\nTo test the app out, follow the consuming data section.\nFurther Configuration Further configuration can be found at: data-connect-trino.\n"},{"url":"/docs/getting-started/clients/","title":"Install Clients","content":"{row-divider}\nInstalling Client Libraries Data Connect has client libraries for R and Python, as well as a command-line interface. We’ll be using these client libraries in the following examples. {divider}  Python R CLI   # Installing the client library form PyPi pip install search-python-client # Installing from Github pip install git+https://github.com/DNAstack/search-python-client --no-cache-dir   # Setup devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;)   This CLI requires Java 11+ on your system\ncurl https://storage.googleapis.com/ga4gh-search-cli/tables-cli-2.1-55-gc484f8b-executable.jar \u0026gt; search-cli chmod +x search-cli mv search-cli /usr/local/bin # (somewhere on your path) search-cli --version You should see:\ntables-api-cli Version : 1.0-0.2.1-55-gc484f8b    \n"},{"url":"/docs/getting-started/provision-data/","title":"Provision Data","content":"{row-divider}\nImplementation Data Connect requires table operations to be implemented to specification for basic discovery and browsing.\nOptional but not required, query operations may be implemented to support querying with SQL.\nThe Data Connect API is backend agnostic, which means any solution that implements the API specification is valid. You can use your favorite backend web application framework to implement Data Connect Endpoints or any HTTPS file server (a cloud blob store, for example) for a tables-in-a-bucket implementation requiring no code.\nCheckout the following examples for some inspiration. {divider} Quick Links  Full API Specifications\nExample Use Cases\n  {row-divider}\nTables-in-a-bucket example The specification allows for a no-code implementation as a collection of files served statically. This is the easiest way to start experimenting with Data Connect. As long as your storage bucket conforms to the correct file structure and it has the correct sharing permissions, it is a valid Data Connect implementation.\nA concrete example implementation is available here and try browsing this implementation with these commands.\n{divider} Here\u0026rsquo;s how you\u0026rsquo;ll need to organize your folders\n tables: served in response to GET /tables table/{table_name}/info: served in response to GET /table/{table_name}/info. e.g. a table with the name mytable should have a corresponding file table/mytable/info table/{table_name}/data: served in response to GET /table/{table_name}/data. e.g. a table with the name mytable should have a corresponding file table/mytable/data table/{table_name}/data_{pageNumber}, which will be linked in the next_page_url of the first table (e.g. mytable). table/{table_name}/data_models/{schemaFile}: Though not required, data models may be linked via $ref. Data models can also be stored as static JSON documents, and be referred to by relative or absolute URLs.   \n{row-divider}\nTry a Reference Implementation Use the following instructions to run a reference Data Connect implementation backed by a publicly accessible Trino instance hosted by DNAstack as the data source.\nYou’ll need Docker set up on your system to run the Spring app and the PostgreSQL database where it stores information about running queries. {divider}  MacOS and Windows Linux   docker pull postgres:latest docker run -d --rm --name dnastack-data-connect-db -e POSTGRES_USER=dataconnecttrino -e POSTGRES_PASSWORD=dataconnecttrino -p 15432:5432 postgres docker pull dnastack/data-connect-trino:latest docker run --rm --name dnastack-data-connect -p 8089:8089 -e TRINO_DATASOURCE_URL=https://trino-public.prod.dnastack.com -e SPRING_DATASOURCE_URL=jdbc:postgresql://host.docker.internal:15432/dataconnecttrino -e SPRING_PROFILES_ACTIVE=no-auth dnastack/data-connect-trino   docker pull postgres:latest docker run -d --rm --name dnastack-data-connect-db -e POSTGRES_USER=dataconnecttrino -e POSTGRES_PASSWORD=dataconnecttrino -p 15432:5432 postgres docker pull dnastack/data-connect-trino:latest docker run --rm --name dnastack-data-connect -p 8089:8089 -e TRINO_DATASOURCE_URL=https://trino-public.prod.dnastack.com -e SPRING_DATASOURCE_URL=jdbc:postgresql://localhost:15432/dataconnecttrino -e SPRING_PROFILES_ACTIVE=no-auth dnastack/data-connect-trino    \n{row-divider}\nOnce you have the Data Connect implementation running, the Data Connect API will be accessible at http://localhost:8089. Here are a few things to try:\n Open http://localhost:8089/tables in your web browser to see the table list API response. It helps if you have a browser plugin that pretty-prints JSON. Try the Python, R, and CLI examples at right. These examples access Data Connect at http://localhost:8089. See the Installing Clients section if you haven\u0026rsquo;t set up the clients yet. Set up your own Trino instance, then re-run the dnastack-data-connect container with the TRINO_DATASOURCE_URL pointed to your own Trino instance.  Further information about this example can be found here. {divider}  Python R CLI   # init search client from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;http://localhost:8089/\u0026#39; search_client = SearchClient(base_url=base_url) # get tables tables_iterator = search_client.get_table_list() tables = [next(tables_iterator, None) for i in range(10)] tables = list(filter(None, tables)) print(tables) # get table info table_name = \u0026#34;sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34; table_info = search_client.get_table_info(table_name) print(table_info) # get table data table_name = \u0026#34;sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34; table_data_iterator = search_client.get_table_data(table_name) table_data = [next(table_data_iterator, None) for i in range(10)] table_data = list(filter(None, table_data)) print(table_data)   # Fetch table list library(httr) tables \u0026lt;- ga4gh.search::ga4gh_list_tables(\u0026#34;http://localhost:8089\u0026#34;) print(tables) # Try a query search_result \u0026lt;- ga4gh.search::ga4gh_search(\u0026#34;http://localhost:8089\u0026#34;, \u0026#34;SELECT sample_phenopackets.ga4gh_tables.gecco_phenopackets\u0026#34;) print(tables)   List tables\nsearch-cli list --api-url http://localhost:8089 Get table info\nsearch-cli info dbgap_demo.scr_gecco_susceptibility.sample_multi --api-url http://localhost:8089 Get table data\nsearch-cli data dbgap_demo.scr_gecco_susceptibility.sample_multi --api-url http://localhost:8089    \n"},{"url":"/docs/getting-started/consume-data/","title":"Consume Data","content":"{row-divider}\nBrowsing At minimum, Data Connect implementations support browsing by table. This means these operations from the API specs are supported for table by table browsing.\nOn the right is example code to browse the tables-in-a-bucket implementation of Data Connect. {divider}  Python R CLI   Follow along in Colab\n# init search client from search_python_client.search import DrsClient, SearchClient base_url_tiab = \u0026#39;https://storage.googleapis.com/ga4gh-tables-example/\u0026#39; search_client_tiab = SearchClient(base_url=base_url_tiab) # get tables tables_iterator = search_client_tiab.get_table_list() tables = [next(tables_iterator, None) for i in range(10)] tables = list(filter(None, tables)) print(tables) # get table info table_name = tables[0][\u0026#39;name\u0026#39;] table_info = search_client_tiab.get_table_info(table_name) print(table_info) # get table data table_name = tables[0][\u0026#39;name\u0026#39;] table_data_iterator = search_client_tiab.get_table_data(table_name) table_data = [next(table_data_iterator, None) for i in range(10)] table_data = list(filter(None, table_data)) print(table_data)   Under construction https://colab.research.google.com/drive/1VOP2IcPjsX4U-DfuiTs7Tr0SVlAD0IMh?usp=sharing \u0026lt;= doesn't work right now.   Get list of tables\nsearch-cli list --api-url https://storage.googleapis.com/ga4gh-tables-example search-cli info subjects --api-url https://storage.googleapis.com/ga4gh-tables-example search-cli data subjects --api-url https://storage.googleapis.com/ga4gh-tables-example   \n{row-divider}\nQueries Data Connect supports query operation through SQL statements.\nData Connect\u0026rsquo;s SQL dialect has a familiar interface inspired by current major open source database platforms, including Trino, PostgreSQL, MySQL, and BigQuery. If you have prior experience with these database platforms, you\u0026rsquo;ll feel right at home with only minor adjustments.\nSupported SQL functions\nSupported SQL grammar\n{divider}  Example #1 Example #2   This query returns all female patients from the patient table.\n/* you can scroll on this tab */ SELECT * FROM kidsfirst.ga4gh_tables.patient WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5;   This query returns all conditions observed in female patients from the patient table.\n/* you can scroll on this tab */ SELECT Json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) AS disease, Json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) AS identifier FROM kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id = REPLACE(Json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5;    \n{row-divider}\nIssuing Queries Using Data Connect Data Connect can be accessed through the straightforward HTTP calls described in its OpenAPI specification.\nWhile Data Connect API can be navigated using programs like cURL or Postman, it is best accessed programmatically. The results could be split into multiple pages, which is easier to navigate with programmatic access.\nFetch each page only once. Data Connect servers are allowed to \u0026ldquo;forget\u0026rdquo; page URLs after you fetch them. This allows the server implementations to be more efficient.\nOn the right, we provide examples to consume data from Data Connect using the GA4GH Commandline Interface, the R client, Python, and cURL.\n Need help installing client libraries?\n {divider}  Python R CLI cURL   Follow Along in Google Colab\n# Installing the client library form PyPi pip install search-python-client # Installing from Github pip install git+https://github.com/DNAstack/search-python-client --no-cache-dir # Building the query from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://search-presto-public.staging.dnastack.com\u0026#39; search_client = SearchClient(base_url=base_url) query = \u0026#34;\u0026#34;\u0026#34; SELECT Json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) AS disease, Json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) AS identifier FROM kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id = REPLACE(Json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE Json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;) = \u0026#39;female\u0026#39; LIMIT 5 \u0026#34;\u0026#34;\u0026#34; # Executing the query table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item) # Results {\u0026#39;disease\u0026#39;: \u0026#39;Aortic atresia\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Aortic atresia|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Mitral atresia\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Mitral atresia|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplasia ascending aorta\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplasia ascending aorta|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplastic left heart syndrome\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplastic left heart syndrome|None\u0026#39;} {\u0026#39;disease\u0026#39;: \u0026#39;Hypoplastic left ventricle (subnormal cavity volume)\u0026#39;, \u0026#39;identifier\u0026#39;: \u0026#39;Condition|SD_PREASA7S|272|Hypoplastic left ventricle (subnormal cavity volume)|None\u0026#39;}   Follow Along in Google Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) conditionsInFemalePatients \u0026lt;- ga4gh.search::ga4gh_search(\u0026#34;https://search-presto-public.staging.dnastack.com\u0026#34;, \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;)=\u0026#39;female\u0026#39; limit 5\u0026#34;) # View the results print(conditionsInFemalePatients) Output:\ndisease 1 Aortic atresia 2 Mitral atresia 3 Hypoplasia ascending aorta 4 Hypoplastic left heart syndrome 5 Hypoplastic left ventricle (subnormal cavity volume) identifier 1 Condition|SD_PREASA7S|272|Aortic atresia|None 2 Condition|SD_PREASA7S|272|Mitral atresia|None 3 Condition|SD_PREASA7S|272|Hypoplasia ascending aorta|None 4 Condition|SD_PREASA7S|272|Hypoplastic left heart syndrome|None 5 Condition|SD_PREASA7S|272|Hypoplastic left ventricle (subnormal cavity volume)|None   search-cli query -q \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;$.code.text\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;$.identifier[0].value\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;$.subject.reference\u0026#39;), \u0026#39;Patient/\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;$.gender\u0026#39;)=\u0026#39;female\u0026#39; limit 5\u0026#34; --api-url https://search-presto-public.staging.dnastack.com   These requests This query returns all female patients from the patient table.\ncurl --request POST \\  --url https://search-presto-public.staging.dnastack.com/search \\  --header \u0026#39;content-type: application/json\u0026#39; \\  --data \u0026#39;{ \u0026#34;query\u0026#34;: \u0026#34;select * from kidsfirst.ga4gh_tables.patient WHERE json_extract_scalar(patient, \u0026#39;\\\u0026#39;\u0026#39;$.gender\u0026#39;\\\u0026#39;\u0026#39;)=\u0026#39;\\\u0026#39;\u0026#39;female\u0026#39;\\\u0026#39;\u0026#39; limit 5\u0026#34;}\u0026#39; This query returns all conditions observed in female patients from the patient table.\ncurl --request POST \\  --url https://search-presto-public.staging.dnastack.com/search \\  --header \u0026#39;content-type: application/json\u0026#39; \\  --data \u0026#39;{ \u0026#34;query\u0026#34;: \u0026#34;select json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.code.text\u0026#39;\\\u0026#39;\u0026#39;) as disease, json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.identifier[0].value\u0026#39;\\\u0026#39;\u0026#39;) as identifier from kidsfirst.ga4gh_tables.ncpi_disease disease INNER JOIN kidsfirst.ga4gh_tables.patient patient ON patient.id=replace(json_extract_scalar(ncpi_disease, \u0026#39;\\\u0026#39;\u0026#39;$.subject.reference\u0026#39;\\\u0026#39;\u0026#39;), \u0026#39;\\\u0026#39;\u0026#39;Patient/\u0026#39;\\\u0026#39;\u0026#39;) WHERE json_extract_scalar(patient, \u0026#39;\\\u0026#39;\u0026#39;$.gender\u0026#39;\\\u0026#39;\u0026#39;)=\u0026#39;\\\u0026#39;\u0026#39;female\u0026#39;\\\u0026#39;\u0026#39; limit 5\u0026#34;}\u0026#39;    \n{row-divider}\nMore Examples dbGaP GECCO Example This is a public implementation of Data Connect. Feel free to follow along with the examples and explore this endpoint with your own script.  Python R CLI   Follow along in Colab\n# init search client from search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://search-presto-public.prod.dnastack.com/\u0026#39; search_client = SearchClient(base_url=base_url) # Find available tables tables_iterator = search_client.get_table_list() tables = list(tables_iterator) import pprint pprint.pprint(tables) #Get more information about a table returned table_info = search_client.get_table_info(\u0026#34;dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi\u0026#34;) pprint.pprint(table_info) # Dig into the table a little further table_data_iterator = search_client.get_table_data(\u0026#34;dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi\u0026#34;) # Limit to first 10 items tables = [next(table_data_iterator, None) for i in range(10)] tables = list(filter(None, tables)) pprint.pprint(tables) # Select all items from the CPS-II study  query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5 \u0026#34;\u0026#34;\u0026#34; # Executing the query table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item)   Follow along in Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) ga4gh.search::ga4gh_list_tables(\u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34;) # Select all items from the CPS-II study  query \u0026lt;- \u0026#34;SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5\u0026#34; # Executing the query ga4gh.search::ga4gh_search(\u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34;, query)   List tables\nsearch-cli list --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; Get table info\nsearch-cli info dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; Now run a query and pipe the results to a file called results.txt\nsearch-cli query -q \u0026#34;SELECT * FROM dbgap_demo.scr_gecco_susceptibility.subject_phenotypes_multi WHERE study = \u0026#39;CPS-II\u0026#39; LIMIT 5\u0026#34; \\  --api-url \u0026#34;https://search-presto-public.prod.dnastack.com\u0026#34; \u0026gt; results.txt   \n{divider}\n COVID Cloud Example This is a public implementation of Data Connect for COVID Cloud. Find more about COVID Cloud here.  Python R CLI   Follow along in Colab\nfrom search_python_client.search import DrsClient, SearchClient base_url = \u0026#39;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com/\u0026#39; search_client = SearchClient(base_url=base_url) # Find available tables tables_iterator = search_client.get_table_list() tables = list(tables_iterator) import pprint pprint.pprint(tables) # Get more information about a table returned table_info = search_client.get_table_info(\u0026#34;covid.cloud.sequences\u0026#34;) pprint.pprint(table_info) # Dig into the table a little further table_data_iterator = search_client.get_table_data(\u0026#34;covid.cloud.sequences\u0026#34;) # Limit to first 10 items tables = [next(table_data_iterator, None) for i in range(1)] tables = list(filter(None, tables)) pprint.pprint(tables) # Select all sequences from GenBank query = \u0026#34;\u0026#34;\u0026#34; SELECT * FROM covid.cloud.sequences WHERE sequence_type=\u0026#39;GenBank\u0026#39; LIMIT 25 \u0026#34;\u0026#34;\u0026#34; table_data_iterator = search_client.search_table(query) for item in table_data_iterator: print(item)   Follow along in Colab\n# installing devtools dir.create(path = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), showWarnings = FALSE, recursive = TRUE) install.packages(\u0026#34;devtools\u0026#34;, lib = Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), repos = \u0026#34;https://cran.rstudio.com/\u0026#34;) # installing the R client devtools::install_github(\u0026#34;DNAstack/ga4gh-search-client-r\u0026#34;) # Making the request library(httr) ga4gh.search::ga4gh_list_tables(\u0026#34;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com\u0026#34;) # Select all data from Genbank. query \u0026lt;- \u0026#34;SELECT * FROM covid.cloud.sequences WHERE sequence_type=\u0026#39;GenBank\u0026#39; LIMIT 25\u0026#34; # Executing the query ga4gh.search::ga4gh_search(\u0026#34;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com\u0026#34;, query)   List tables\nsearch-cli list --api-url \u0026#34;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com\u0026#34; Get table info\nsearch-cli info covid.cloud.sequences --api-url \u0026#34;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com\u0026#34; Now run a query and pipe the results to a file called results.txt\nsearch-cli query -q \u0026#34;SELECT * FROM covid.cloud.sequences WHERE sequence_type=\u0026#39;GenBank\u0026#39; LIMIT 25\u0026#34; \\  --api-url \u0026#34;https://ga4gh-search-adapter-presto-covid19-public.prod.dnastack.com\u0026#34; \u0026gt; results.txt   \n "}]